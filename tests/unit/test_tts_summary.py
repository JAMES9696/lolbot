import sys
import types

import pytest

# æä¾›æœ€å°åŒ–çš„ google.generativeai stubï¼Œé¿å…åœ¨å•å…ƒæµ‹è¯•ä¸­å¼•å…¥çœŸå®ä¾èµ–ã€‚
if "google.generativeai" not in sys.modules:
    google_module = types.ModuleType("google")
    generative_module = types.ModuleType("google.generativeai")
    generative_module.GenerativeModel = object  # type: ignore[attr-defined]

    def _noop_configure(*_args, **_kwargs):  # noqa: D401
        return None

    generative_module.configure = _noop_configure  # type: ignore[attr-defined]
    google_module.generativeai = generative_module  # type: ignore[attr-defined]
    sys.modules["google"] = google_module
    sys.modules["google.generativeai"] = generative_module

from src.contracts.analysis_results import V1ScoreSummary
from src.tasks import analysis_tasks


class _StubGemini:
    """Stub adapter returning verbose markdown-like narrative."""

    async def analyze_match(self, payload, prompt):  # noqa: D401
        return (
            "# ğŸ™ï¸ èµ›åè¯­éŸ³æ’­æŠ¥\n\n"
            "**ç»¼åˆè¯„åˆ†**ï¼š36/100ï¼Œè¡¨ç°æ¬ ä½³ã€‚\n"
            "æ ¸å¿ƒå¼±ç‚¹ï¼šç”Ÿå­˜ç‡è¿‡ä½ï¼Œ9 æ¬¡é˜µäº¡å¯¼è‡´è¿ç»­æ·˜æ±°ã€‚\n"
            "å»ºè®®ï¼šæ™š 3 ç§’å…¥åœºï¼Œä¸“æ³¨æ®‹è¡€åˆ‡å…¥ã€‚"
        )


@pytest.mark.asyncio
async def test_tts_summary_preserves_natural_paragraphs() -> None:
    """TTS æ‘˜è¦åº”å»é™¤ Markdown å™ªéŸ³ï¼Œä½†ä¿ç•™è‡ªç„¶è¯­æ°”ä¸æ®µè½ã€‚"""
    adapter = _StubGemini()
    summary = V1ScoreSummary(
        combat_score=36.5,
        economy_score=33.3,
        vision_score=0.0,
        objective_score=0.0,
        teamplay_score=55.5,
        growth_score=53.0,
        tankiness_score=85.0,
        damage_composition_score=65.1,
        survivability_score=29.7,
        cc_contribution_score=28.4,
        overall_score=36.0,
        raw_stats={"placement": 7, "kills": 1, "deaths": 9, "assists": 5},
    )

    outcome = await analysis_tasks._generate_tts_summary(  # type: ignore[attr-defined]
        adapter,
        "dummy narrative",
        summary,
        "é—æ†¾",
        "Irelia",
        "Arena",
    )

    assert outcome is not None
    assert outcome.source == "llm"
    tts_text = outcome.text
    assert tts_text.startswith("ğŸ™ï¸ èµ›åè¯­éŸ³æ’­æŠ¥")
    assert "ç»¼åˆè¯„åˆ†ï¼š36/100" in tts_text
    assert "æ ¸å¿ƒå¼±ç‚¹ï¼šç”Ÿå­˜ç‡è¿‡ä½" in tts_text
    assert "å»ºè®®ï¼šæ™š 3 ç§’å…¥åœº" in tts_text
    assert "#" not in tts_text
    assert "- " not in tts_text
    assert tts_text.count("ã€‚") >= 3


class _HallucinatingGemini:
    """è¿”å›å«â€œæ•°æ®ç¼ºå¤±â€æç¤ºçš„æ–‡æ¡ˆï¼›æ”¾å®½ç­–ç•¥ä¸‹ä¸åº”æŠ›å¼‚å¸¸ã€‚"""

    async def analyze_match(self, payload, prompt):  # noqa: D401
        return (
            "ğŸ“Š è‹±é›„è”ç›Ÿæˆ˜æŠ¥åˆ†æ\n"
            "âš ï¸ æ•°æ®çŠ¶æ€å¼‚å¸¸\n"
            "æ— æ³•ç”Ÿæˆæˆ˜æŠ¥ | æ¯”èµ›æ•°æ®å®Œå…¨ç¼ºå¤± | å»ºè®®ï¼šè¯·æ£€æŸ¥æ¯”èµ›IDæ˜¯å¦æ­£ç¡®æˆ–ç¨åé‡è¯•APIè·å–\n"
        )


@pytest.mark.asyncio
async def test_tts_summary_hallucination_becomes_soft_ok() -> None:
    """LLM å‡ºç°"æ•°æ®ç¼ºå¤±"æç¤ºæ—¶ï¼Œä¸åº”æŠ›å¼‚å¸¸ï¼›åº”å›é€€åˆ° fallback æ–‡æœ¬ã€‚"""
    adapter = _HallucinatingGemini()
    summary = V1ScoreSummary(
        combat_score=72.0,
        economy_score=60.0,
        vision_score=55.0,
        objective_score=58.0,
        teamplay_score=50.0,
        growth_score=40.0,
        tankiness_score=62.0,
        damage_composition_score=68.0,
        survivability_score=45.0,
        cc_contribution_score=70.0,
        overall_score=61.0,
        raw_stats={"placement": 4},
    )

    outcome = await analysis_tasks._generate_tts_summary(  # type: ignore[attr-defined]
        adapter,
        "dummy narrative",
        summary,
        "é—æ†¾",
        "Irelia",
        "Arena",
    )

    assert outcome is not None
    # After hallucination detection, system now degrades to fallback instead of sanitizing LLM output
    assert outcome.source == "fallback"
    assert "æ— æ³•ç”Ÿæˆ" not in outcome.text
    assert outcome.text.strip(), "Fallback è¾“å‡ºåº”åŒ…å«å¯æ’­æŠ¥æ–‡æœ¬"
    valid, hints = analysis_tasks._validate_tts_candidate(outcome.text)  # type: ignore[attr-defined]
    assert valid, f"Fallback æ–‡æœ¬åº”é€šè¿‡æ ¡éªŒ hints={hints}"
    assert "fallback_used" in (outcome.soft_hints or ())


def test_tts_summary_detects_data_load_hallucination() -> None:
    """åŒ…å«"æ•°æ®åŠ è½½"ç­‰æªè¾çš„æ‘˜è¦ä¼šè¢« _sanitize_tts_summary æ¸…é™¤ï¼Œå¯¼è‡´æ–‡æœ¬è¿‡çŸ­è€Œæ— æ•ˆã€‚"""
    # The raw candidate contains banned phrase
    candidate_with_hallucination = "å…„å¼Ÿä»¬ï¼Œè¿™å±€æ¯”èµ›æ•°æ®åŠ è½½é‡åˆ°äº†ç‚¹å°é—®é¢˜ï¼Œä½†æˆ‘ä»¬å…ˆèŠèŠå¤ç›˜ã€‚"

    # After sanitization, banned tokens are removed
    sanitized = analysis_tasks._sanitize_tts_summary(candidate_with_hallucination)  # type: ignore[attr-defined]
    # "æ•°æ®åŠ è½½" is removed, leaving: "å…„å¼Ÿä»¬ï¼Œè¿™å±€æ¯”èµ›é‡åˆ°äº†ç‚¹å°é—®é¢˜ï¼Œä½†æˆ‘ä»¬å…ˆèŠèŠå¤ç›˜ã€‚"
    assert "æ•°æ®åŠ è½½" not in sanitized

    # The sanitized text should now be too short (< 60 chars)
    valid, hints = analysis_tasks._validate_tts_candidate(sanitized)  # type: ignore[attr-defined]
    assert not valid
    assert "too_short" in hints


class _RecordingGemini:
    """è®°å½• prompt ä»¥éªŒè¯ä¸Šä¸‹æ–‡æ˜¯å¦æ³¨å…¥çœŸå®æ•°æ®ã€‚"""

    def __init__(self) -> None:
        self.prompts: list[str] = []

    async def analyze_match(self, payload, prompt):  # noqa: D401
        self.prompts.append(prompt)
        # Return a valid TTS summary (3 sentences, proper length)
        return (
            "Oriannaè¿™å±€ç»¼åˆ51åˆ†ï¼Œå±€é¢ä¸åˆ©ï¼Œéœ€è¦ç¨³ä½èŠ‚å¥å…ˆä¿å‘è‚²ã€‚"
            "äº®ç‚¹è½åœ¨ç»æµ77åˆ†ï¼Œè¯´æ˜æ ¸å¿ƒæ“ä½œä»åœ¨çº¿ï¼Œè¦ç»§ç»­æŠŠæ¡è¿™ä¸€é¡¹ä¼˜åŠ¿ã€‚"
            "çŸ­æ¿å¤„åœ¨å›¢é˜Ÿ47åˆ†ï¼Œå¤šè·Ÿæ‰“é‡æŠ¥ç‚¹ï¼Œç­‰é˜Ÿå‹å¼€å›¢å†è·Ÿè¿›ï¼Œå¹¶åŠæ—¶ä¸é˜Ÿå‹æ²Ÿé€šä¸‹ä¸€æ³¢è®¡åˆ’ã€‚"
        )


@pytest.mark.asyncio
async def test_tts_summary_prompt_includes_structured_context() -> None:
    """ç”ŸæˆTTSæ‘˜è¦æ—¶åº”å‘LLMæä¾›çœŸå®æ•°æ®ä¸Šä¸‹æ–‡ï¼Œé™ä½èƒ¡è¯´æ¦‚ç‡ã€‚"""
    adapter = _RecordingGemini()
    summary = V1ScoreSummary(
        combat_score=58.0,
        economy_score=77.3,
        vision_score=22.0,
        objective_score=15.5,
        teamplay_score=46.7,
        growth_score=68.0,
        tankiness_score=62.0,
        damage_composition_score=66.7,
        survivability_score=54.0,
        cc_contribution_score=88.0,
        overall_score=50.7,
        raw_stats={
            "kills": 6,
            "deaths": 4,
            "assists": 6,
            "cs": 286,
            "cs_per_min": 8.0,
            "vision_score": 19,
            "damage_dealt": 43742,
            "damage_taken": 22107,
            "cc_time": 510.0,
            "cc_per_min": 14.6,
            "sr_enrichment": {"duration_min": 35.0},
        },
    )

    outcome = await analysis_tasks._generate_tts_summary(  # type: ignore[attr-defined]
        adapter,
        "å®Œæ•´å™äº‹æ–‡æœ¬",
        summary,
        "é—æ†¾",
        "Orianna",
        "SR",
    )

    assert adapter.prompts, "åº”è‡³å°‘è°ƒç”¨ä¸€æ¬¡ Gemini analyze_match"
    prompt = adapter.prompts[-1]
    assert "=== æˆ˜å±€éª¨æ¶" in prompt
    assert "æˆ˜åœºåŸºè°ƒ" in prompt
    assert "å¯å¼•ç”¨æ•°å­—" in prompt
    # Prompt wording changed from "æœ€å¤šæŒ‘å…¶ä¸­ä¸€åˆ°ä¸¤ä¸ª" to "æ’­æŠ¥æ—¶æŒ‘ä¸€ä¸¤ä¸ªå³å¯"
    assert "æ’­æŠ¥æ—¶æŒ‘" in prompt or "æŒ‘ä¸€ä¸¤ä¸ª" in prompt
    assert "K/D/A" in prompt
    # CC data is deliberately excluded from TTS context per architectural decision
    # (see analysis_tasks.py line 1962-2043 for full rationale)
    assert "æ§åˆ¶æ—¶é—´" not in prompt and "æ§åˆ¶å æ¯”" not in prompt
    assert outcome is not None
    assert outcome.source == "llm"


@pytest.mark.asyncio
async def test_tts_summary_prompt_includes_objective_breakdown() -> None:
    """SR èµ›å±€åº”åœ¨ prompt ä¸­åŒ…å«ç›®æ ‡è½¬æ¢ä¸æ‹†å¡”æ•°æ®ã€‚"""
    adapter = _RecordingGemini()
    summary = V1ScoreSummary(
        combat_score=60.0,
        economy_score=70.0,
        vision_score=45.0,
        objective_score=35.0,
        teamplay_score=55.0,
        growth_score=50.0,
        tankiness_score=40.0,
        damage_composition_score=68.0,
        survivability_score=44.0,
        cc_contribution_score=30.0,
        overall_score=52.0,
        raw_stats={
            "kills": 5,
            "deaths": 6,
            "assists": 8,
            "cs": 210,
            "cs_per_min": 7.0,
            "vision_score": 23,
            "damage_dealt": 19876,
            "damage_taken": 22110,
            "sr_enrichment": {
                "objective_breakdown": {"towers": 1, "drakes": 2, "heralds": 1, "barons": 0},
                "post_kill_objective_conversions": 3,
                "team_kills_considered": 7,
                "conversion_rate": 0.428,
                "gold_diff_10": 320,
                "xp_diff_10": 110,
                "duration_min": 28.5,
            },
        },
    )

    await analysis_tasks._generate_tts_summary(  # type: ignore[attr-defined]
        adapter,
        "å™äº‹æ–‡æœ¬",
        summary,
        "é—æ†¾",
        "Kai'Sa",
        "SR",
    )

    assert adapter.prompts, "åº”è®°å½•ç”Ÿæˆ prompt"
    prompt = adapter.prompts[-1]
    assert "èµ„æºæˆ˜æœ" in prompt
    assert "æ¨å¡” 1" in prompt
    assert "æ§é¾™ 2" in prompt
    assert "å…ˆé”‹ 1" in prompt
    assert "å‡»æ€è½¬ç›®æ ‡" in prompt
    assert "æˆåŠŸç‡çº¦ 42%" in prompt
    assert "GoldÎ”10" not in prompt
    assert "XPÎ”10" not in prompt


@pytest.mark.asyncio
async def test_tts_summary_high_conversion_adds_positive_tone_hint() -> None:
    """é«˜è½¬åŒ–ç‡æ—¶ï¼Œåº”æ³¨å…¥æ­£å‘è¯­æ°”è½¯æç¤ºé¿å…æ¶ˆææªè¾ã€‚"""
    adapter = _RecordingGemini()
    summary = V1ScoreSummary(
        combat_score=55.0,
        economy_score=62.0,
        vision_score=40.0,
        objective_score=48.0,
        teamplay_score=58.0,
        growth_score=52.0,
        tankiness_score=37.0,
        damage_composition_score=61.0,
        survivability_score=49.0,
        cc_contribution_score=42.0,
        overall_score=51.0,
        raw_stats={
            "kills": 7,
            "deaths": 3,
            "assists": 9,
            "cs": 240,
            "cs_per_min": 6.8,
            "sr_enrichment": {
                "objective_breakdown": {"towers": 2, "drakes": 2},
                "post_kill_objective_conversions": 8,
                "team_kills_considered": 10,
                "conversion_rate": 0.82,
                "duration_min": 30.0,
            },
        },
    )

    await analysis_tasks._generate_tts_summary(  # type: ignore[attr-defined]
        adapter,
        "æ•´å±€æè¿°",
        summary,
        "å†·é™",
        "Sylas",
        "SR",
    )

    assert adapter.prompts, "é«˜è½¬åŒ–ç‡åœºæ™¯åº”ç”Ÿæˆæ’­æŠ¥ prompt"
    prompt = adapter.prompts[-1]
    assert "è¯­æ°”æç¤º" in prompt
    assert "é¿å…ä½¿ç”¨â€œåªâ€â€œä»…â€" in prompt
    assert "è½¬ç›®æ ‡æ•ˆç‡" in prompt


class _OverlongGemini:
    """è¿”å›è¶…è¿‡ 5 å¥çš„é•¿æ–‡æœ¬ï¼ŒéªŒè¯å‹ç¼©é€»è¾‘ã€‚"""

    async def analyze_match(self, payload, prompt):  # noqa: D401
        return (
            "ç¬¬ä¸€å¥è¯¦è¿°å‰æœŸä¼˜åŠ¿ã€‚ç¬¬äºŒå¥ç»§ç»­å™è¿°å›¢æˆ˜ã€‚ç¬¬ä¸‰å¥è®¨è®ºè§†é‡ã€‚"
            "ç¬¬å››å¥åˆ†æç»æµã€‚ç¬¬äº”å¥è®²åˆ°ç›®æ ‡æ§åˆ¶ã€‚ç¬¬å…­å¥è°ˆç”Ÿå­˜é—®é¢˜ã€‚"
        )


@pytest.mark.asyncio
async def test_tts_summary_trims_llm_sentences_to_three() -> None:
    """å½“ LLM äº§å‡ºè¶…è¿‡ 3 å¥æ—¶ï¼Œåº”å‹ç¼©ä¸º 3 å¥ä»¥å†…ã€‚"""
    adapter = _OverlongGemini()
    summary = _make_summary_for_fallback()

    outcome = await analysis_tasks._generate_tts_summary(  # type: ignore[attr-defined]
        adapter,
        "dummy narrative",
        summary,
        "é—æ†¾",
        "å¡è",
        "SR",
    )

    assert outcome is not None
    sentences = [seg for seg in outcome.text.split("ã€‚") if seg.strip()]
    assert len(sentences) == 3
    assert len(outcome.text) <= analysis_tasks._TTS_MAX_CHARS  # type: ignore[attr-defined]


def _make_summary_for_fallback() -> V1ScoreSummary:
    return V1ScoreSummary(
        combat_score=58.0,
        economy_score=73.0,
        vision_score=18.0,
        objective_score=25.0,
        teamplay_score=40.0,
        growth_score=24.0,
        tankiness_score=74.0,
        damage_composition_score=67.0,
        survivability_score=41.0,
        cc_contribution_score=33.0,
        overall_score=46.0,
        raw_stats={"champion_name": "å¡è", "game_mode": "SR"},
    )


def test_tts_fallback_meets_minimum_contract() -> None:
    """Fallback æ–‡æœ¬å¿…é¡»æ»¡è¶³é•¿åº¦ä¸å¥å­çº¦æŸï¼Œé¿å…å†æ¬¡è§¦å‘éªŒè¯å¤±è´¥ã€‚"""
    summary = _make_summary_for_fallback()
    fallback_text = analysis_tasks._build_tts_fallback(summary, "å¡è", "SR")  # type: ignore[attr-defined]
    processed = analysis_tasks._sanitize_tts_summary(fallback_text, summary)  # type: ignore[attr-defined]
    processed = analysis_tasks._compress_tts_text(
        processed.strip(),
        analysis_tasks._TTS_MAX_CHARS,  # type: ignore[attr-defined]
    )
    assert len(processed) >= analysis_tasks._TTS_MIN_CHARS  # type: ignore[attr-defined]
    is_valid, hints = analysis_tasks._validate_tts_candidate(processed)  # type: ignore[attr-defined]
    assert is_valid, f"Fallback should pass validator hints={hints}"


class _ExplodingGemini:
    """Simulate LLM API error that raises generic Exception."""

    async def analyze_match(self, payload, prompt):  # noqa: D401
        raise RuntimeError("Simulated LLM API timeout")


@pytest.mark.asyncio
async def test_tts_summary_exception_is_caught_and_raised() -> None:
    """_generate_tts_summary should raise Exception when LLM fails unexpectedly.

    This test verifies that _generate_tts_summary doesn't swallow generic exceptions,
    allowing Stage 4.5's exception handler to catch them and gracefully degrade.
    """
    adapter = _ExplodingGemini()
    summary = _make_summary_for_fallback()

    with pytest.raises(RuntimeError, match="Simulated LLM API timeout"):
        await analysis_tasks._generate_tts_summary(  # type: ignore[attr-defined]
            adapter,
            "dummy narrative",
            summary,
            "é—æ†¾",
            "å¡è",
            "SR",
        )
