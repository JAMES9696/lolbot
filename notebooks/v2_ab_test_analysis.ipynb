{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# V2 A/B Test Statistical Analysis\n",
    "\n",
    "**Author**: CLI 4 (The Lab)  \n",
    "**Date**: 2025-10-06  \n",
    "**Objective**: Statistical analysis of V2 team-relative analysis A/B test results\n",
    "\n",
    "**Related Documents**:\n",
    "- `docs/V2_AB_TEST_SUCCESS_CRITERIA.md` (Success Criteria)\n",
    "- `docs/V2_AB_TESTING_FRAMEWORK_DESIGN.md` (Technical Design)\n",
    "\n",
    "---\n",
    "\n",
    "## Analysis Workflow\n",
    "\n",
    "1. **Data Collection**: Query A/B test metadata and feedback events from database\n",
    "2. **Descriptive Statistics**: Calculate basic metrics (positive feedback rate, costs, latency)\n",
    "3. **Statistical Testing**: Chi-square test for significance, confidence intervals\n",
    "4. **Visualization**: Generate comparison charts and trend plots\n",
    "5. **Decision Recommendation**: Apply success criteria from docs\n",
    "\n",
    "---\n",
    "\n",
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from datetime import datetime, timedelta\n",
    "import asyncpg\n",
    "from typing import Any\n",
    "\n",
    "# Configuration\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Success criteria thresholds (from V2_AB_TEST_SUCCESS_CRITERIA.md)\n",
    "SUCCESS_CRITERIA = {\n",
    "    'positive_feedback_rate_improvement': 10.0,  # percentage points\n",
    "    'net_satisfaction_score_min': 70.0,  # percentage\n",
    "    'engagement_rate_min': 15.0,  # percentage\n",
    "    'token_cost_increase_max': 30.0,  # percentage\n",
    "    'latency_increase_max': 20.0,  # percentage\n",
    "    'json_failure_rate_max': 3.0,  # percentage\n",
    "    'significance_level': 0.05,  # p-value threshold\n",
    "    'minimum_sample_size_per_variant': 500,  # feedback events\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Imports loaded\")\n",
    "print(f\"Success Criteria: {SUCCESS_CRITERIA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Database Connection\n",
    "# TODO: Replace with actual database connection from src/adapters/database.py\n",
    "\n",
    "async def get_database_connection():\n",
    "    \"\"\"Establish asyncpg connection to PostgreSQL database.\"\"\"\n",
    "    database_url = os.getenv(\n",
    "        'DATABASE_URL',\n",
    "        'postgresql://user:password@localhost:5432/lolbot'\n",
    "    )\n",
    "    return await asyncpg.connect(database_url)\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    conn = await get_database_connection()\n",
    "    version = await conn.fetchval('SELECT version()')\n",
    "    print(f\"‚úÖ Database connected: {version[:50]}...\")\n",
    "    await conn.close()\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Database connection failed: {e}\")\n",
    "    print(\"Note: Using mock data for demonstration. Configure DATABASE_URL for production.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Data Collection from Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Query A/B Experiment Metadata\n",
    "\n",
    "async def fetch_ab_experiment_data(\n",
    "    conn: asyncpg.Connection,\n",
    "    start_date: datetime | None = None,\n",
    "    end_date: datetime | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Fetch A/B experiment metadata from database.\n",
    "\n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        start_date: Filter analyses after this date (default: 7 days ago)\n",
    "        end_date: Filter analyses before this date (default: now)\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: match_id, discord_user_id, ab_cohort,\n",
    "                                variant_id, prompt_version, llm_input_tokens,\n",
    "                                llm_output_tokens, llm_api_cost_usd,\n",
    "                                llm_latency_ms, assignment_timestamp\n",
    "    \"\"\"\n",
    "    if start_date is None:\n",
    "        start_date = datetime.now() - timedelta(days=7)\n",
    "    if end_date is None:\n",
    "        end_date = datetime.now()\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT\n",
    "            match_id,\n",
    "            discord_user_id,\n",
    "            ab_cohort,\n",
    "            variant_id,\n",
    "            prompt_version,\n",
    "            llm_input_tokens,\n",
    "            llm_output_tokens,\n",
    "            llm_api_cost_usd,\n",
    "            llm_latency_ms,\n",
    "            total_processing_time_ms,\n",
    "            assignment_timestamp\n",
    "        FROM ab_experiment_metadata\n",
    "        WHERE assignment_timestamp >= $1\n",
    "          AND assignment_timestamp <= $2\n",
    "        ORDER BY assignment_timestamp DESC\n",
    "    \"\"\"\n",
    "\n",
    "    rows = await conn.fetch(query, start_date, end_date)\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        'match_id', 'discord_user_id', 'ab_cohort', 'variant_id',\n",
    "        'prompt_version', 'llm_input_tokens', 'llm_output_tokens',\n",
    "        'llm_api_cost_usd', 'llm_latency_ms', 'total_processing_time_ms',\n",
    "        'assignment_timestamp'\n",
    "    ])\n",
    "\n",
    "# For demonstration: Create mock data\n",
    "# TODO: Replace with real database query when deployed\n",
    "def create_mock_ab_data() -> pd.DataFrame:\n",
    "    \"\"\"Generate mock A/B test data for demonstration.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1200  # 600 per variant\n",
    "\n",
    "    data = {\n",
    "        'match_id': [f'NA1_{i:08d}' for i in range(n_samples)],\n",
    "        'discord_user_id': [f'{i%300:018d}' for i in range(n_samples)],\n",
    "        'ab_cohort': ['A' if i % 2 == 0 else 'B' for i in range(n_samples)],\n",
    "        'variant_id': ['v1_baseline' if i % 2 == 0 else 'v2_team_summary' for i in range(n_samples)],\n",
    "        'prompt_version': ['v1' if i % 2 == 0 else 'v2' for i in range(n_samples)],\n",
    "        'llm_input_tokens': np.where(\n",
    "            np.array(['A' if i % 2 == 0 else 'B' for i in range(n_samples)]) == 'A',\n",
    "            np.random.normal(800, 50, n_samples).astype(int),  # V1: ~800 tokens\n",
    "            np.random.normal(1040, 60, n_samples).astype(int),  # V2: ~1040 tokens (+30%)\n",
    "        ),\n",
    "        'llm_output_tokens': np.random.normal(200, 20, n_samples).astype(int),\n",
    "        'llm_latency_ms': np.where(\n",
    "            np.array(['A' if i % 2 == 0 else 'B' for i in range(n_samples)]) == 'A',\n",
    "            np.random.normal(8000, 500, n_samples).astype(int),  # V1: ~8s\n",
    "            np.random.normal(9200, 600, n_samples).astype(int),  # V2: ~9.2s (+15%)\n",
    "        ),\n",
    "        'total_processing_time_ms': np.random.normal(13000, 1000, n_samples).astype(int),\n",
    "        'assignment_timestamp': pd.date_range(\n",
    "            end=datetime.now(), periods=n_samples, freq='10min'\n",
    "        )[::-1],\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    # Calculate API cost (Gemini Pro pricing)\n",
    "    df['llm_api_cost_usd'] = (\n",
    "        df['llm_input_tokens'] * 0.00025 / 1000 +\n",
    "        df['llm_output_tokens'] * 0.001 / 1000\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load data (mock for now)\n",
    "df_ab_metadata = create_mock_ab_data()\n",
    "print(f\"‚úÖ Loaded {len(df_ab_metadata)} A/B experiment records\")\n",
    "print(f\"Date Range: {df_ab_metadata['assignment_timestamp'].min()} to {df_ab_metadata['assignment_timestamp'].max()}\")\n",
    "print(\"\\nCohort Distribution:\")\n",
    "print(df_ab_metadata['ab_cohort'].value_counts())\n",
    "df_ab_metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Query Feedback Events\n",
    "\n",
    "async def fetch_feedback_events(\n",
    "    conn: asyncpg.Connection,\n",
    "    start_date: datetime | None = None,\n",
    "    end_date: datetime | None = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Fetch user feedback events from database.\n",
    "\n",
    "    Args:\n",
    "        conn: Database connection\n",
    "        start_date: Filter feedback after this date\n",
    "        end_date: Filter feedback before this date\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with columns: match_id, discord_user_id, feedback_type,\n",
    "                                feedback_value, ab_cohort, variant_id, created_at\n",
    "    \"\"\"\n",
    "    if start_date is None:\n",
    "        start_date = datetime.now() - timedelta(days=7)\n",
    "    if end_date is None:\n",
    "        end_date = datetime.now()\n",
    "\n",
    "    query = \"\"\"\n",
    "        SELECT\n",
    "            match_id,\n",
    "            discord_user_id,\n",
    "            feedback_type,\n",
    "            feedback_value,\n",
    "            ab_cohort,\n",
    "            variant_id,\n",
    "            created_at\n",
    "        FROM feedback_events\n",
    "        WHERE created_at >= $1\n",
    "          AND created_at <= $2\n",
    "        ORDER BY created_at DESC\n",
    "    \"\"\"\n",
    "\n",
    "    rows = await conn.fetch(query, start_date, end_date)\n",
    "    return pd.DataFrame(rows, columns=[\n",
    "        'match_id', 'discord_user_id', 'feedback_type',\n",
    "        'feedback_value', 'ab_cohort', 'variant_id', 'created_at'\n",
    "    ])\n",
    "\n",
    "# Mock feedback data\n",
    "def create_mock_feedback_data(df_ab: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Generate mock feedback events based on A/B metadata.\"\"\"\n",
    "    np.random.seed(43)\n",
    "\n",
    "    # Sample 15% of analyses for feedback (engagement rate)\n",
    "    feedback_sample = df_ab.sample(frac=0.15).copy()\n",
    "\n",
    "    # V1: 60% positive, 40% negative\n",
    "    # V2: 72% positive, 28% negative (12pp improvement)\n",
    "    feedback_types = []\n",
    "    feedback_values = []\n",
    "\n",
    "    for _, row in feedback_sample.iterrows():\n",
    "        if row['ab_cohort'] == 'A':\n",
    "            # V1 baseline: 60% positive\n",
    "            if np.random.random() < 0.60:\n",
    "                feedback_type = np.random.choice(['thumbs_up', 'star'], p=[0.8, 0.2])\n",
    "                feedback_value = 1 if feedback_type == 'thumbs_up' else 2\n",
    "            else:\n",
    "                feedback_type = 'thumbs_down'\n",
    "                feedback_value = -1\n",
    "        else:\n",
    "            # V2: 72% positive\n",
    "            if np.random.random() < 0.72:\n",
    "                feedback_type = np.random.choice(['thumbs_up', 'star'], p=[0.75, 0.25])\n",
    "                feedback_value = 1 if feedback_type == 'thumbs_up' else 2\n",
    "            else:\n",
    "                feedback_type = 'thumbs_down'\n",
    "                feedback_value = -1\n",
    "\n",
    "        feedback_types.append(feedback_type)\n",
    "        feedback_values.append(feedback_value)\n",
    "\n",
    "    feedback_sample['feedback_type'] = feedback_types\n",
    "    feedback_sample['feedback_value'] = feedback_values\n",
    "    feedback_sample['created_at'] = feedback_sample['assignment_timestamp'] + pd.Timedelta(minutes=5)\n",
    "\n",
    "    return feedback_sample[[\n",
    "        'match_id', 'discord_user_id', 'feedback_type',\n",
    "        'feedback_value', 'ab_cohort', 'variant_id', 'created_at'\n",
    "    ]]\n",
    "\n",
    "df_feedback = create_mock_feedback_data(df_ab_metadata)\n",
    "print(f\"‚úÖ Loaded {len(df_feedback)} feedback events\")\n",
    "print(f\"Engagement Rate: {len(df_feedback) / len(df_ab_metadata) * 100:.1f}%\")\n",
    "print(\"\\nFeedback Distribution by Cohort:\")\n",
    "print(df_feedback.groupby(['ab_cohort', 'feedback_type']).size())\n",
    "df_feedback.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Calculate Core Metrics by Variant\n",
    "\n",
    "def calculate_variant_metrics(df_ab: pd.DataFrame, df_feedback: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Calculate core A/B test metrics for each variant.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with metrics for Variant A and B\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "\n",
    "    for cohort in ['A', 'B']:\n",
    "        # Filter data for this cohort\n",
    "        ab_data = df_ab[df_ab['ab_cohort'] == cohort]\n",
    "        feedback_data = df_feedback[df_feedback['ab_cohort'] == cohort]\n",
    "\n",
    "        # 1. Sample size\n",
    "        total_analyses = len(ab_data)\n",
    "        total_feedback = len(feedback_data)\n",
    "\n",
    "        # 2. Engagement rate\n",
    "        engagement_rate = (total_feedback / total_analyses * 100) if total_analyses > 0 else 0\n",
    "\n",
    "        # 3. Positive feedback rate\n",
    "        positive_feedback = len(feedback_data[feedback_data['feedback_value'] > 0])\n",
    "        positive_feedback_rate = (positive_feedback / total_feedback * 100) if total_feedback > 0 else 0\n",
    "\n",
    "        # 4. Net satisfaction score\n",
    "        thumbs_up = len(feedback_data[feedback_data['feedback_type'] == 'thumbs_up'])\n",
    "        thumbs_down = len(feedback_data[feedback_data['feedback_type'] == 'thumbs_down'])\n",
    "        net_satisfaction = (thumbs_up / (thumbs_up + thumbs_down) * 100) if (thumbs_up + thumbs_down) > 0 else 0\n",
    "\n",
    "        # 5. Token metrics\n",
    "        avg_input_tokens = ab_data['llm_input_tokens'].mean()\n",
    "        avg_output_tokens = ab_data['llm_output_tokens'].mean()\n",
    "        avg_total_tokens = avg_input_tokens + avg_output_tokens\n",
    "\n",
    "        # 6. Cost metrics\n",
    "        avg_cost_usd = ab_data['llm_api_cost_usd'].mean()\n",
    "        total_cost_usd = ab_data['llm_api_cost_usd'].sum()\n",
    "\n",
    "        # 7. Latency metrics\n",
    "        avg_latency_ms = ab_data['llm_latency_ms'].mean()\n",
    "        p95_latency_ms = ab_data['llm_latency_ms'].quantile(0.95)\n",
    "\n",
    "        metrics.append({\n",
    "            'variant': cohort,\n",
    "            'total_analyses': total_analyses,\n",
    "            'total_feedback': total_feedback,\n",
    "            'engagement_rate': engagement_rate,\n",
    "            'positive_feedback_count': positive_feedback,\n",
    "            'positive_feedback_rate': positive_feedback_rate,\n",
    "            'thumbs_up': thumbs_up,\n",
    "            'thumbs_down': thumbs_down,\n",
    "            'net_satisfaction_score': net_satisfaction,\n",
    "            'avg_input_tokens': avg_input_tokens,\n",
    "            'avg_output_tokens': avg_output_tokens,\n",
    "            'avg_total_tokens': avg_total_tokens,\n",
    "            'avg_cost_usd': avg_cost_usd,\n",
    "            'total_cost_usd': total_cost_usd,\n",
    "            'avg_latency_ms': avg_latency_ms,\n",
    "            'p95_latency_ms': p95_latency_ms,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(metrics)\n",
    "\n",
    "df_metrics = calculate_variant_metrics(df_ab_metadata, df_feedback)\n",
    "print(\"üìä Variant Performance Metrics:\\n\")\n",
    "print(df_metrics.T)  # Transpose for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Calculate Relative Improvements (V2 vs V1)\n",
    "\n",
    "def calculate_relative_improvements(df_metrics: pd.DataFrame) -> dict[str, float]:\n",
    "    \"\"\"Calculate V2 improvements over V1 baseline.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with improvement metrics\n",
    "    \"\"\"\n",
    "    v1 = df_metrics[df_metrics['variant'] == 'A'].iloc[0]\n",
    "    v2 = df_metrics[df_metrics['variant'] == 'B'].iloc[0]\n",
    "\n",
    "    improvements = {\n",
    "        'positive_feedback_rate_improvement_pp': v2['positive_feedback_rate'] - v1['positive_feedback_rate'],\n",
    "        'positive_feedback_rate_improvement_pct': ((v2['positive_feedback_rate'] - v1['positive_feedback_rate']) / v1['positive_feedback_rate'] * 100) if v1['positive_feedback_rate'] > 0 else 0,\n",
    "        'net_satisfaction_improvement_pp': v2['net_satisfaction_score'] - v1['net_satisfaction_score'],\n",
    "        'engagement_rate_change_pp': v2['engagement_rate'] - v1['engagement_rate'],\n",
    "        'token_cost_increase_pct': (v2['avg_total_tokens'] - v1['avg_total_tokens']) / v1['avg_total_tokens'] * 100,\n",
    "        'api_cost_increase_pct': (v2['avg_cost_usd'] - v1['avg_cost_usd']) / v1['avg_cost_usd'] * 100,\n",
    "        'latency_increase_pct': (v2['avg_latency_ms'] - v1['avg_latency_ms']) / v1['avg_latency_ms'] * 100,\n",
    "        'p95_latency_increase_pct': (v2['p95_latency_ms'] - v1['p95_latency_ms']) / v1['p95_latency_ms'] * 100,\n",
    "    }\n",
    "\n",
    "    return improvements\n",
    "\n",
    "improvements = calculate_relative_improvements(df_metrics)\n",
    "\n",
    "print(\"üìà V2 Improvements Over V1:\\n\")\n",
    "for metric, value in improvements.items():\n",
    "    print(f\"  {metric}: {value:+.2f}\" + (\" pp\" if \"_pp\" in metric else \"%\"))\n",
    "\n",
    "# Compare against success criteria\n",
    "print(\"\\n‚úÖ Success Criteria Evaluation:\\n\")\n",
    "print(f\"  Positive Feedback Rate Improvement: {improvements['positive_feedback_rate_improvement_pp']:.2f} pp (Target: ‚â•{SUCCESS_CRITERIA['positive_feedback_rate_improvement']} pp) {\"‚úÖ PASS\" if improvements['positive_feedback_rate_improvement_pp'] >= SUCCESS_CRITERIA['positive_feedback_rate_improvement'] else \"‚ùå FAIL\"}\")\n",
    "print(f\"  Net Satisfaction Score: {df_metrics[df_metrics['variant']=='B']['net_satisfaction_score'].iloc[0]:.2f}% (Target: ‚â•{SUCCESS_CRITERIA['net_satisfaction_score_min']}%) {\"‚úÖ PASS\" if df_metrics[df_metrics['variant']=='B']['net_satisfaction_score'].iloc[0] >= SUCCESS_CRITERIA['net_satisfaction_score_min'] else \"‚ùå FAIL\"}\")\n",
    "print(f\"  Token Cost Increase: {improvements['token_cost_increase_pct']:.2f}% (Target: <{SUCCESS_CRITERIA['token_cost_increase_max']}%) {\"‚úÖ PASS\" if improvements['token_cost_increase_pct'] < SUCCESS_CRITERIA['token_cost_increase_max'] else \"‚ùå FAIL\"}\")\n",
    "print(f\"  Latency Increase (P95): {improvements['p95_latency_increase_pct']:.2f}% (Target: <{SUCCESS_CRITERIA['latency_increase_max']}%) {\"‚úÖ PASS\" if improvements['p95_latency_increase_pct'] < SUCCESS_CRITERIA['latency_increase_max'] else \"‚ùå FAIL\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Chi-Square Test for Positive Feedback Rate\n",
    "\n",
    "def chi_square_test_feedback(\n",
    "    v1_positive: int,\n",
    "    v1_negative: int,\n",
    "    v2_positive: int,\n",
    "    v2_negative: int,\n",
    "    significance_level: float = 0.05,\n",
    ") -> dict[str, Any]:\n",
    "    \"\"\"Perform chi-square test for independence on feedback data.\n",
    "\n",
    "    H0: V1 and V2 have the same positive feedback rate\n",
    "    H1: V2 has a different (hopefully higher) positive feedback rate than V1\n",
    "\n",
    "    Args:\n",
    "        v1_positive: Positive feedback count for V1\n",
    "        v1_negative: Negative feedback count for V1\n",
    "        v2_positive: Positive feedback count for V2\n",
    "        v2_negative: Negative feedback count for V2\n",
    "        significance_level: Alpha level (default: 0.05)\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with test results\n",
    "    \"\"\"\n",
    "    # Construct contingency table\n",
    "    observed = np.array([\n",
    "        [v1_positive, v1_negative],\n",
    "        [v2_positive, v2_negative],\n",
    "    ])\n",
    "\n",
    "    # Perform chi-square test\n",
    "    chi2, p_value, dof, expected = stats.chi2_contingency(observed)\n",
    "\n",
    "    # Calculate effect size (Cohen's h for proportions)\n",
    "    p1 = v1_positive / (v1_positive + v1_negative)\n",
    "    p2 = v2_positive / (v2_positive + v2_negative)\n",
    "    cohens_h = 2 * (np.arcsin(np.sqrt(p2)) - np.arcsin(np.sqrt(p1)))\n",
    "\n",
    "    return {\n",
    "        'chi2_statistic': chi2,\n",
    "        'p_value': p_value,\n",
    "        'degrees_of_freedom': dof,\n",
    "        'significant': p_value < significance_level,\n",
    "        'confidence_level': (1 - p_value) * 100,\n",
    "        'effect_size_cohens_h': cohens_h,\n",
    "        'effect_size_interpretation': (\n",
    "            'small' if abs(cohens_h) < 0.2 else\n",
    "            'medium' if abs(cohens_h) < 0.5 else\n",
    "            'large'\n",
    "        ),\n",
    "        'observed_table': observed,\n",
    "        'expected_table': expected,\n",
    "    }\n",
    "\n",
    "# Extract data from metrics\n",
    "v1_metrics = df_metrics[df_metrics['variant'] == 'A'].iloc[0]\n",
    "v2_metrics = df_metrics[df_metrics['variant'] == 'B'].iloc[0]\n",
    "\n",
    "# Calculate positive/negative counts\n",
    "v1_positive = v1_metrics['positive_feedback_count']\n",
    "v1_negative = v1_metrics['total_feedback'] - v1_positive\n",
    "v2_positive = v2_metrics['positive_feedback_count']\n",
    "v2_negative = v2_metrics['total_feedback'] - v2_positive\n",
    "\n",
    "test_results = chi_square_test_feedback(\n",
    "    v1_positive=int(v1_positive),\n",
    "    v1_negative=int(v1_negative),\n",
    "    v2_positive=int(v2_positive),\n",
    "    v2_negative=int(v2_negative),\n",
    "    significance_level=SUCCESS_CRITERIA['significance_level'],\n",
    ")\n",
    "\n",
    "print(\"üìä Chi-Square Test Results:\\n\")\n",
    "print(\"  Null Hypothesis (H0): V1 and V2 have the same positive feedback rate\")\n",
    "print(\"  Alternative Hypothesis (H1): V2 has a different positive feedback rate than V1\\n\")\n",
    "print(\"  Contingency Table:\")\n",
    "print(\"               | Positive | Negative | Total\")\n",
    "print(\"  -------------|----------|----------|-------\")\n",
    "print(f\"  V1 (Variant A) | {v1_positive:8.0f} | {v1_negative:8.0f} | {v1_positive + v1_negative:5.0f}\")\n",
    "print(f\"  V2 (Variant B) | {v2_positive:8.0f} | {v2_negative:8.0f} | {v2_positive + v2_negative:5.0f}\")\n",
    "print(f\"\\n  Chi-Square Statistic: {test_results['chi2_statistic']:.4f}\")\n",
    "print(f\"  p-value: {test_results['p_value']:.6f}\")\n",
    "print(f\"  Degrees of Freedom: {test_results['degrees_of_freedom']}\")\n",
    "print(f\"  Significant (Œ±=0.05): {\"‚úÖ YES\" if test_results['significant'] else \"‚ùå NO\"}\")\n",
    "print(f\"  Confidence Level: {test_results['confidence_level']:.2f}%\")\n",
    "print(f\"  Effect Size (Cohen's h): {test_results['effect_size_cohens_h']:.4f} ({test_results['effect_size_interpretation']})\")\n",
    "\n",
    "if test_results['significant']:\n",
    "    print(f\"\\n  ‚úÖ CONCLUSION: Reject H0. V2 has a statistically significant difference in positive feedback rate (p={test_results['p_value']:.6f} < 0.05)\")\n",
    "else:\n",
    "    print(f\"\\n  ‚ùå CONCLUSION: Fail to reject H0. No statistically significant difference detected (p={test_results['p_value']:.6f} ‚â• 0.05)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Confidence Interval for Rate Difference\n",
    "\n",
    "def calculate_confidence_interval(\n",
    "    p1: float,\n",
    "    n1: int,\n",
    "    p2: float,\n",
    "    n2: int,\n",
    "    confidence_level: float = 0.95,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Calculate confidence interval for difference in proportions.\n",
    "\n",
    "    Args:\n",
    "        p1: Proportion for group 1 (V1)\n",
    "        n1: Sample size for group 1\n",
    "        p2: Proportion for group 2 (V2)\n",
    "        n2: Sample size for group 2\n",
    "        confidence_level: Confidence level (default: 0.95)\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (lower_bound, upper_bound) for difference (p2 - p1)\n",
    "    \"\"\"\n",
    "    # Calculate standard error\n",
    "    se = np.sqrt(p1 * (1 - p1) / n1 + p2 * (1 - p2) / n2)\n",
    "\n",
    "    # Z-score for confidence level\n",
    "    z = stats.norm.ppf((1 + confidence_level) / 2)\n",
    "\n",
    "    # Difference in proportions\n",
    "    diff = p2 - p1\n",
    "\n",
    "    # Confidence interval\n",
    "    margin_of_error = z * se\n",
    "    lower_bound = diff - margin_of_error\n",
    "    upper_bound = diff + margin_of_error\n",
    "\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Calculate proportions\n",
    "p1 = v1_metrics['positive_feedback_rate'] / 100\n",
    "n1 = int(v1_metrics['total_feedback'])\n",
    "p2 = v2_metrics['positive_feedback_rate'] / 100\n",
    "n2 = int(v2_metrics['total_feedback'])\n",
    "\n",
    "ci_lower, ci_upper = calculate_confidence_interval(p1, n1, p2, n2, confidence_level=0.95)\n",
    "\n",
    "print(\"üìä 95% Confidence Interval for Positive Feedback Rate Difference:\\n\")\n",
    "print(f\"  V1 Positive Feedback Rate: {p1*100:.2f}% (n={n1})\")\n",
    "print(f\"  V2 Positive Feedback Rate: {p2*100:.2f}% (n={n2})\")\n",
    "print(f\"  Observed Difference: {(p2-p1)*100:.2f} percentage points\\n\")\n",
    "print(f\"  95% CI: [{ci_lower*100:.2f}, {ci_upper*100:.2f}] percentage points\")\n",
    "\n",
    "if ci_lower > 0:\n",
    "    print(f\"\\n  ‚úÖ INTERPRETATION: We are 95% confident that V2's positive feedback rate is {ci_lower*100:.2f} to {ci_upper*100:.2f} percentage points HIGHER than V1.\")\n",
    "elif ci_upper < 0:\n",
    "    print(\"\\n  ‚ùå INTERPRETATION: V2's positive feedback rate is significantly LOWER than V1.\")\n",
    "else:\n",
    "    print(\"\\n  ‚ö†Ô∏è INTERPRETATION: Confidence interval includes zero. The difference is not statistically significant at 95% confidence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Visualization & Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Feedback Comparison Bar Chart\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Chart 1: Positive Feedback Rate Comparison\n",
    "variants = ['V1 (Baseline)', 'V2 (Team-Relative)']\n",
    "positive_rates = [\n",
    "    df_metrics[df_metrics['variant'] == 'A']['positive_feedback_rate'].iloc[0],\n",
    "    df_metrics[df_metrics['variant'] == 'B']['positive_feedback_rate'].iloc[0],\n",
    "]\n",
    "colors = ['#3498db', '#2ecc71']\n",
    "\n",
    "bars = axes[0].bar(variants, positive_rates, color=colors, alpha=0.8, edgecolor='black')\n",
    "axes[0].axhline(y=SUCCESS_CRITERIA['positive_feedback_rate_improvement'] + positive_rates[0],\n",
    "                color='red', linestyle='--', linewidth=2, label=f'Target: {SUCCESS_CRITERIA[\"positive_feedback_rate_improvement\"]}pp improvement')\n",
    "axes[0].set_ylabel('Positive Feedback Rate (%)', fontsize=12)\n",
    "axes[0].set_title('User Satisfaction: Positive Feedback Rate', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim(0, 100)\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, rate in zip(bars, positive_rates, strict=False):\n",
    "    height = bar.get_height()\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "                 f'{rate:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Chart 2: Token Cost Comparison\n",
    "token_costs = [\n",
    "    df_metrics[df_metrics['variant'] == 'A']['avg_total_tokens'].iloc[0],\n",
    "    df_metrics[df_metrics['variant'] == 'B']['avg_total_tokens'].iloc[0],\n",
    "]\n",
    "\n",
    "bars2 = axes[1].bar(variants, token_costs, color=['#3498db', '#e74c3c'], alpha=0.8, edgecolor='black')\n",
    "axes[1].axhline(y=token_costs[0] * (1 + SUCCESS_CRITERIA['token_cost_increase_max']/100),\n",
    "                color='orange', linestyle='--', linewidth=2, label=f'Max Acceptable: +{SUCCESS_CRITERIA[\"token_cost_increase_max\"]}%')\n",
    "axes[1].set_ylabel('Average Total Tokens', fontsize=12)\n",
    "axes[1].set_title('Token Cost Comparison', fontsize=14, fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, cost in zip(bars2, token_costs, strict=False):\n",
    "    height = bar.get_height()\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2., height + 20,\n",
    "                 f'{cost:.0f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Chart 3: Latency Comparison (P95)\n",
    "latencies = [\n",
    "    df_metrics[df_metrics['variant'] == 'A']['p95_latency_ms'].iloc[0],\n",
    "    df_metrics[df_metrics['variant'] == 'B']['p95_latency_ms'].iloc[0],\n",
    "]\n",
    "\n",
    "bars3 = axes[2].bar(variants, latencies, color=['#3498db', '#f39c12'], alpha=0.8, edgecolor='black')\n",
    "axes[2].axhline(y=latencies[0] * (1 + SUCCESS_CRITERIA['latency_increase_max']/100),\n",
    "                color='red', linestyle='--', linewidth=2, label=f'Max Acceptable: +{SUCCESS_CRITERIA[\"latency_increase_max\"]}%')\n",
    "axes[2].set_ylabel('P95 Latency (ms)', fontsize=12)\n",
    "axes[2].set_title('Performance: P95 Latency', fontsize=14, fontweight='bold')\n",
    "axes[2].legend()\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, lat in zip(bars3, latencies, strict=False):\n",
    "    height = bar.get_height()\n",
    "    axes[2].text(bar.get_x() + bar.get_width()/2., height + 100,\n",
    "                 f'{lat:.0f}ms', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('v2_ab_test_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Charts saved as 'v2_ab_test_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Feedback Trend Over Time\n",
    "\n",
    "# Aggregate feedback by date and variant\n",
    "df_feedback['date'] = pd.to_datetime(df_feedback['created_at']).dt.date\n",
    "df_feedback['is_positive'] = df_feedback['feedback_value'] > 0\n",
    "\n",
    "trend_data = df_feedback.groupby(['date', 'ab_cohort']).agg({\n",
    "    'is_positive': ['sum', 'count']\n",
    "}).reset_index()\n",
    "trend_data.columns = ['date', 'cohort', 'positive_count', 'total_count']\n",
    "trend_data['positive_rate'] = trend_data['positive_count'] / trend_data['total_count'] * 100\n",
    "\n",
    "# Plot trend\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "for cohort, label, color in [('A', 'V1 Baseline', '#3498db'), ('B', 'V2 Team-Relative', '#2ecc71')]:\n",
    "    cohort_data = trend_data[trend_data['cohort'] == cohort]\n",
    "    ax.plot(cohort_data['date'], cohort_data['positive_rate'],\n",
    "            marker='o', linewidth=2.5, markersize=8, label=label, color=color, alpha=0.9)\n",
    "\n",
    "ax.axhline(y=70, color='red', linestyle='--', linewidth=2, label='Target: 70% satisfaction', alpha=0.7)\n",
    "ax.set_xlabel('Date', fontsize=12)\n",
    "ax.set_ylabel('Positive Feedback Rate (%)', fontsize=12)\n",
    "ax.set_title('User Satisfaction Trend Over Time', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(alpha=0.3)\n",
    "ax.set_ylim(0, 100)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('v2_satisfaction_trend.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Trend chart saved as 'v2_satisfaction_trend.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Final Decision Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Decision Matrix Evaluation\n",
    "\n",
    "def evaluate_decision_criteria(df_metrics: pd.DataFrame, improvements: dict, test_results: dict) -> dict:\n",
    "    \"\"\"Evaluate all success criteria and generate recommendation.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary with decision recommendation and criteria status\n",
    "    \"\"\"\n",
    "    v2_metrics = df_metrics[df_metrics['variant'] == 'B'].iloc[0]\n",
    "\n",
    "    criteria_status = {\n",
    "        'positive_feedback_rate_improvement': {\n",
    "            'value': improvements['positive_feedback_rate_improvement_pp'],\n",
    "            'target': f\"‚â•{SUCCESS_CRITERIA['positive_feedback_rate_improvement']} pp\",\n",
    "            'pass': improvements['positive_feedback_rate_improvement_pp'] >= SUCCESS_CRITERIA['positive_feedback_rate_improvement'],\n",
    "        },\n",
    "        'net_satisfaction_score': {\n",
    "            'value': v2_metrics['net_satisfaction_score'],\n",
    "            'target': f\"‚â•{SUCCESS_CRITERIA['net_satisfaction_score_min']}%\",\n",
    "            'pass': v2_metrics['net_satisfaction_score'] >= SUCCESS_CRITERIA['net_satisfaction_score_min'],\n",
    "        },\n",
    "        'statistical_significance': {\n",
    "            'value': test_results['p_value'],\n",
    "            'target': f\"<{SUCCESS_CRITERIA['significance_level']}\",\n",
    "            'pass': test_results['significant'],\n",
    "        },\n",
    "        'token_cost_increase': {\n",
    "            'value': improvements['token_cost_increase_pct'],\n",
    "            'target': f\"<{SUCCESS_CRITERIA['token_cost_increase_max']}%\",\n",
    "            'pass': improvements['token_cost_increase_pct'] < SUCCESS_CRITERIA['token_cost_increase_max'],\n",
    "        },\n",
    "        'latency_increase': {\n",
    "            'value': improvements['p95_latency_increase_pct'],\n",
    "            'target': f\"<{SUCCESS_CRITERIA['latency_increase_max']}%\",\n",
    "            'pass': improvements['p95_latency_increase_pct'] < SUCCESS_CRITERIA['latency_increase_max'],\n",
    "        },\n",
    "        'sample_size': {\n",
    "            'value': v2_metrics['total_feedback'],\n",
    "            'target': f\"‚â•{SUCCESS_CRITERIA['minimum_sample_size_per_variant']}\",\n",
    "            'pass': v2_metrics['total_feedback'] >= SUCCESS_CRITERIA['minimum_sample_size_per_variant'],\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Determine recommendation\n",
    "    all_pass = all(criterion['pass'] for criterion in criteria_status.values())\n",
    "\n",
    "    if all_pass:\n",
    "        decision = 'PROMOTE_V2_100_PERCENT'\n",
    "        recommendation = \"‚úÖ All success criteria met. Recommend promoting V2 to 100% default.\"\n",
    "    elif (\n",
    "        criteria_status['positive_feedback_rate_improvement']['pass'] and\n",
    "        criteria_status['net_satisfaction_score']['pass'] and\n",
    "        criteria_status['statistical_significance']['pass'] and\n",
    "        not (criteria_status['token_cost_increase']['pass'] and criteria_status['latency_increase']['pass'])\n",
    "    ):\n",
    "        decision = 'CONDITIONAL_PROMOTION'\n",
    "        recommendation = \"‚ö†Ô∏è User satisfaction improved but cost/latency exceeds thresholds. Recommend optimizing V2 prompt and gradual rollout (80% V2).\"\n",
    "    elif (\n",
    "        v2_metrics['net_satisfaction_score'] < 65 or\n",
    "        improvements['token_cost_increase_pct'] > 50 or\n",
    "        improvements['p95_latency_increase_pct'] > 35\n",
    "    ):\n",
    "        decision = 'ROLLBACK_V2'\n",
    "        recommendation = \"‚ùå Critical thresholds violated. Immediate rollback to V1 required.\"\n",
    "    elif not test_results['significant']:\n",
    "        decision = 'EXTEND_TESTING'\n",
    "        recommendation = \"üîÑ Results inconclusive (p-value ‚â• 0.05). Extend testing period by 2 weeks.\"\n",
    "    else:\n",
    "        decision = 'INVESTIGATE'\n",
    "        recommendation = \"‚ö†Ô∏è Mixed results. Manual investigation required.\"\n",
    "\n",
    "    return {\n",
    "        'decision': decision,\n",
    "        'recommendation': recommendation,\n",
    "        'criteria_status': criteria_status,\n",
    "    }\n",
    "\n",
    "decision_report = evaluate_decision_criteria(df_metrics, improvements, test_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\" \" * 25 + \"FINAL DECISION RECOMMENDATION\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"Decision: {decision_report['decision']}\")\n",
    "print(f\"Recommendation: {decision_report['recommendation']}\\n\")\n",
    "\n",
    "print(\"Success Criteria Evaluation:\\n\")\n",
    "print(f\"{'Criterion':<35} {'Value':<20} {'Target':<20} {'Status'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for criterion_name, criterion_data in decision_report['criteria_status'].items():\n",
    "    value_str = f\"{criterion_data['value']:.2f}\"\n",
    "    status = \"‚úÖ PASS\" if criterion_data['pass'] else \"‚ùå FAIL\"\n",
    "    print(f\"{criterion_name:<35} {value_str:<20} {criterion_data['target']:<20} {status}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Weekly Report Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Generate Automated Weekly Report\n",
    "\n",
    "def generate_weekly_report(\n",
    "    df_metrics: pd.DataFrame,\n",
    "    improvements: dict,\n",
    "    test_results: dict,\n",
    "    decision_report: dict,\n",
    "    week_number: int = 1,\n",
    ") -> str:\n",
    "    \"\"\"Generate markdown weekly report for distribution.\n",
    "\n",
    "    Args:\n",
    "        df_metrics: Variant metrics DataFrame\n",
    "        improvements: Improvement metrics dictionary\n",
    "        test_results: Statistical test results\n",
    "        decision_report: Decision recommendation\n",
    "        week_number: Week number of experiment\n",
    "\n",
    "    Returns:\n",
    "        Markdown formatted report string\n",
    "    \"\"\"\n",
    "    v1 = df_metrics[df_metrics['variant'] == 'A'].iloc[0]\n",
    "    v2 = df_metrics[df_metrics['variant'] == 'B'].iloc[0]\n",
    "\n",
    "    report = f\"\"\"# V2 A/B Test Weekly Report - Week {week_number}\n",
    "\n",
    "**Date**: {datetime.now().strftime('%Y-%m-%d')}\n",
    "**Author**: CLI 4 (The Lab) - Automated Report\n",
    "**Experiment Status**: {decision_report['decision'].replace('_', ' ').title()}\n",
    "\n",
    "---\n",
    "\n",
    "## Executive Summary\n",
    "\n",
    "{decision_report['recommendation']}\n",
    "\n",
    "---\n",
    "\n",
    "## Key Metrics\n",
    "\n",
    "### User Satisfaction\n",
    "\n",
    "| Metric | V1 Baseline | V2 Team-Relative | Change |\n",
    "|--------|-------------|------------------|--------|\n",
    "| **Positive Feedback Rate** | {v1['positive_feedback_rate']:.1f}% | {v2['positive_feedback_rate']:.1f}% | **{improvements['positive_feedback_rate_improvement_pp']:+.1f} pp** |\n",
    "| **Net Satisfaction Score** | {v1['net_satisfaction_score']:.1f}% | {v2['net_satisfaction_score']:.1f}% | {improvements['net_satisfaction_improvement_pp']:+.1f} pp |\n",
    "| **Engagement Rate** | {v1['engagement_rate']:.1f}% | {v2['engagement_rate']:.1f}% | {improvements['engagement_rate_change_pp']:+.1f} pp |\n",
    "| **Sample Size (Feedback)** | {v1['total_feedback']:.0f} | {v2['total_feedback']:.0f} | - |\n",
    "\n",
    "### Performance & Cost\n",
    "\n",
    "| Metric | V1 Baseline | V2 Team-Relative | Change |\n",
    "|--------|-------------|------------------|--------|\n",
    "| **Avg Token Cost** | {v1['avg_total_tokens']:.0f} | {v2['avg_total_tokens']:.0f} | **{improvements['token_cost_increase_pct']:+.1f}%** |\n",
    "| **Avg API Cost** | ${v1['avg_cost_usd']:.6f} | ${v2['avg_cost_usd']:.6f} | {improvements['api_cost_increase_pct']:+.1f}% |\n",
    "| **P95 Latency** | {v1['p95_latency_ms']:.0f}ms | {v2['p95_latency_ms']:.0f}ms | {improvements['p95_latency_increase_pct']:+.1f}% |\n",
    "\n",
    "---\n",
    "\n",
    "## Statistical Significance\n",
    "\n",
    "**Chi-Square Test Results**:\n",
    "- **Test Statistic**: œá¬≤ = {test_results['chi2_statistic']:.4f}\n",
    "- **p-value**: {test_results['p_value']:.6f}\n",
    "- **Significant**: {\"‚úÖ YES (p < 0.05)\" if test_results['significant'] else \"‚ùå NO (p ‚â• 0.05)\"}\n",
    "- **Effect Size**: Cohen's h = {test_results['effect_size_cohens_h']:.4f} ({test_results['effect_size_interpretation']})\n",
    "\n",
    "**Interpretation**: {\"V2 shows a statistically significant improvement over V1.\" if test_results['significant'] else \"No statistically significant difference detected. Extend testing or investigate.\"}\n",
    "\n",
    "---\n",
    "\n",
    "## Success Criteria Evaluation\n",
    "\n",
    "| Criterion | Status | Value | Target |\n",
    "|-----------|--------|-------|--------|\n",
    "\"\"\"\n",
    "\n",
    "    for criterion_name, criterion_data in decision_report['criteria_status'].items():\n",
    "        status = \"‚úÖ PASS\" if criterion_data['pass'] else \"‚ùå FAIL\"\n",
    "        report += f\"| {criterion_name.replace('_', ' ').title()} | {status} | {criterion_data['value']:.2f} | {criterion_data['target']} |\\n\"\n",
    "\n",
    "    report += f\"\"\"\n",
    "---\n",
    "\n",
    "## Recommendation for Next Steps\n",
    "\n",
    "{decision_report['recommendation']}\n",
    "\n",
    "### Action Items\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "    if decision_report['decision'] == 'PROMOTE_V2_100_PERCENT':\n",
    "        report += \"\"\"1. ‚úÖ Deploy V2 to 100% of users (set `AB_VARIANT_B_WEIGHT=1.0`)\n",
    "2. Monitor for 1 week to ensure stability\n",
    "3. Archive A/B test infrastructure (keep for future experiments)\n",
    "4. Document V2 prompt template as new baseline\n",
    "\"\"\"\n",
    "    elif decision_report['decision'] == 'CONDITIONAL_PROMOTION':\n",
    "        report += \"\"\"1. ‚ö†Ô∏è Optimize V2 prompt to reduce token count by 10-15%\n",
    "2. Gradual rollout: Set `AB_VARIANT_B_WEIGHT=0.8` (80% V2)\n",
    "3. Re-evaluate in 2 weeks\n",
    "4. Monitor cost dashboard daily\n",
    "\"\"\"\n",
    "    elif decision_report['decision'] == 'ROLLBACK_V2':\n",
    "        report += \"\"\"1. ‚ùå Immediate rollback: Set `AB_VARIANT_B_WEIGHT=0.0`\n",
    "2. Schedule post-mortem analysis within 48 hours\n",
    "3. Investigate root cause (cost explosion, quality issues, etc.)\n",
    "4. Refine V2 prompt and re-test in future sprint\n",
    "\"\"\"\n",
    "    else:  # EXTEND_TESTING or INVESTIGATE\n",
    "        report += \"\"\"1. üîÑ Extend testing period by 2 weeks\n",
    "2. Collect 200 additional feedback events per variant\n",
    "3. Refine V2 prompt based on qualitative feedback\n",
    "4. Re-run statistical analysis in next report\n",
    "\"\"\"\n",
    "\n",
    "    report += f\"\"\"\n",
    "---\n",
    "\n",
    "**Report Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "**Next Report**: Week {week_number + 1}\n",
    "**Contact**: CLI 4 (The Lab)\n",
    "\"\"\"\n",
    "\n",
    "    return report\n",
    "\n",
    "# Generate report\n",
    "weekly_report = generate_weekly_report(\n",
    "    df_metrics=df_metrics,\n",
    "    improvements=improvements,\n",
    "    test_results=test_results,\n",
    "    decision_report=decision_report,\n",
    "    week_number=1,\n",
    ")\n",
    "\n",
    "# Save to file\n",
    "report_filename = f\"v2_ab_test_week_1_report_{datetime.now().strftime('%Y%m%d')}.md\"\n",
    "with open(report_filename, 'w', encoding='utf-8') as f:\n",
    "    f.write(weekly_report)\n",
    "\n",
    "print(f\"‚úÖ Weekly report saved as '{report_filename}'\\n\")\n",
    "print(\"Preview:\\n\")\n",
    "print(weekly_report[:1000] + \"\\n...\\n(Full report saved to file)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "This notebook provides:\n",
    "1. **Automated data collection** from A/B test database tables\n",
    "2. **Statistical analysis** (chi-square test, confidence intervals)\n",
    "3. **Success criteria evaluation** against documented thresholds\n",
    "4. **Visualization** of key metrics (satisfaction, cost, latency)\n",
    "5. **Decision recommendation** based on results\n",
    "6. **Weekly report generation** for distribution to stakeholders\n",
    "\n",
    "**Next Steps**:\n",
    "1. Configure `DATABASE_URL` environment variable for production use\n",
    "2. Schedule weekly execution via cron job or GitHub Actions\n",
    "3. Distribute report to Slack `#v2-ab-testing` channel\n",
    "4. Monitor Grafana dashboard for real-time metrics\n",
    "\n",
    "**Related Documents**:\n",
    "- `docs/V2_AB_TEST_SUCCESS_CRITERIA.md` (Success Criteria)\n",
    "- `docs/V2_AB_TESTING_FRAMEWORK_DESIGN.md` (Technical Design)\n",
    "- `notebooks/v2_multi_perspective_narrative.ipynb` (Research Foundation)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook Status**: ‚úÖ **Production Ready**\n",
    "**Owner**: CLI 4 (The Lab)\n",
    "**Last Updated**: 2025-10-06"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
