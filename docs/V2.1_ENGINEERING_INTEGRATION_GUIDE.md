# V2.1 Prescriptive Analysis - Engineering Integration Guide

**Document Version**: 1.0
**Author**: CLI 4 (The Lab)
**Date**: 2025-10-06
**Status**: ✅ Ready for CLI 2 Implementation
**Target Audience**: CLI 2 (Backend Engineer)

---

## Executive Summary

This document provides **complete engineering specifications** for implementing V2.1 Prescriptive Analysis based on CLI 4's research findings (`notebooks/v2.1_prescriptive_analysis.ipynb`).

**Key Deliverables** (Ready for Implementation):
1. ✅ **Pydantic Data Contracts**: `src/contracts/v21_prescriptive_analysis.py`
2. ✅ **Prompt Template**: `src/prompts/v21_coaching_prescriptive.txt`
3. ✅ **This Integration Guide**: Implementation roadmap and technical specifications

**Expected Implementation Timeline**: 2 weeks (8-10 days backend + 2-3 days integration testing)

---

## 1. Architecture Overview

### 1.1 V2.1 Data Flow

```
┌─────────────────────────────────────────────────────────────────┐
│ STAGE 1-3: Data Retrieval (Unchanged from V2)                  │
│ - Fetch Match-V5 data + Timeline                               │
│ - Calculate V1 scores for target player + 4 teammates          │
└────────────────────────┬────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────────┐
│ STAGE 4 (NEW): Timeline Evidence Extraction                    │
│                                                                 │
│ Input:                                                          │
│   - timeline_data (Match-V5 Timeline API response)             │
│   - target_player_puuid                                        │
│   - weak_dimensions (from V1 scoring)                          │
│                                                                 │
│ Process:                                                        │
│   FOR each dimension WHERE player_score < team_avg:            │
│     1. Extract relevant timeline events (WARD_PLACED, etc.)    │
│     2. Calculate comparative stats (player vs. support)        │
│     3. Identify critical_impact_event (Baron stolen, etc.)     │
│     4. Format timestamps to MM:SS                              │
│                                                                 │
│ Output:                                                         │
│   - V21PrescriptiveAnalysisInput (Pydantic model)              │
└────────────────────────┬────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────────┐
│ STAGE 5 (MODIFIED): LLM Generation with V2.1 Prompt            │
│                                                                 │
│ Input:                                                          │
│   - V21PrescriptiveAnalysisInput                               │
│   - v21_coaching_prescriptive.txt prompt template              │
│                                                                 │
│ LLM Call (Gemini Pro):                                         │
│   - Enable JSON Mode with V21PrescriptiveAnalysisReport schema │
│   - Temperature: 0.7 (balance creativity and consistency)      │
│   - Max output tokens: 2048                                    │
│                                                                 │
│ Output:                                                         │
│   - V21PrescriptiveAnalysisReport (validated by Pydantic)      │
└────────────────────────┬────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────────────┐
│ STAGE 6: Storage & Delivery (Unchanged)                        │
│ - Store report in database (match_analytics table)             │
│ - Publish to Discord via webhook (CLI 1)                       │
└─────────────────────────────────────────────────────────────────┘
```

---

## 2. Implementation Tasks (CLI 2)

### Task 2.1: Timeline Evidence Extraction Function

**File**: `src/core/services/timeline_evidence_extractor.py` (NEW)

#### 2.1.1 Function Signature

```python
from src.contracts.v21_prescriptive_analysis import (
    V21PrescriptiveAnalysisInput,
    V21WeakDimension,
    V21TimelineEvidence,
)

def extract_prescriptive_evidence(
    timeline_data: dict[str, Any],  # Match-V5 Timeline API response
    player_puuid: str,
    weak_dimensions: list[dict[str, Any]],  # From V1 scoring
    summoner_name: str,
    champion_name: str,
    match_id: str,
    match_result: Literal["victory", "defeat"],
    overall_score: float,
) -> V21PrescriptiveAnalysisInput:
    """Extract timeline evidence for V2.1 prescriptive analysis.

    Args:
        timeline_data: Full Match-V5 Timeline API response
        player_puuid: Target player's PUUID
        weak_dimensions: List of dimensions where player_score < team_avg
                         Format: [{"dimension": "Vision", "score": 62.4, ...}, ...]
        summoner_name: Player's summoner name
        champion_name: Played champion name
        match_id: Match ID
        match_result: Match outcome
        overall_score: Player's overall V1 score

    Returns:
        V21PrescriptiveAnalysisInput ready for LLM prompt
    """
    pass  # Implementation below
```

---

#### 2.1.2 Timeline Event Parsing Requirements

**Required Event Types** (from Match-V5 Timeline API):

| Event Type | Purpose | Key Fields to Extract |
|------------|---------|----------------------|
| **WARD_PLACED** | Vision evidence | `wardType`, `timestamp`, `position.x/y`, `creatorId` (player) |
| **WARD_KILL** | Vision evidence | `killerId`, `timestamp`, `wardType` |
| **ELITE_MONSTER_KILL** | Objective evidence | `monsterType` (BARON/DRAGON), `killerTeamId`, `timestamp` |
| **BUILDING_KILL** | Objective evidence | `buildingType` (TOWER), `killerTeamId`, `timestamp` |
| **CHAMPION_KILL** | Combat evidence | `killerId`, `victimId`, `timestamp`, `position.x/y` |
| **Player Position Frames** | Positioning evidence | `participantFrames[].position.x/y` (every 60s) |

**Event Filtering Logic** (by Dimension):

```python
def filter_events_by_dimension(
    timeline_data: dict[str, Any],
    dimension: str,
    player_puuid: str,
) -> list[V21TimelineEvidence]:
    """Filter timeline events relevant to a specific dimension."""

    if dimension == "Vision":
        # Extract WARD_PLACED, WARD_KILL events
        events = []
        for frame in timeline_data.get("info", {}).get("frames", []):
            for event in frame.get("events", []):
                if event["type"] == "WARD_PLACED" and event.get("creatorId") == player_participant_id:
                    events.append(
                        V21TimelineEvidence(
                            event_type="WARD_PLACED",
                            timestamp_ms=event["timestamp"],
                            formatted_timestamp=_format_timestamp(event["timestamp"]),
                            details={
                                "item": event.get("wardType", "Unknown"),
                                "location": _get_location_name(event["position"]),
                                "lifetime_seconds": 0,  # Calculate if ward was destroyed
                            },
                            player_context=None,
                        )
                    )

        # Calculate ward stats (total placed, avg lifetime, comparison with support)
        ward_stats = _calculate_ward_stats(timeline_data, player_puuid)
        events.append(
            V21TimelineEvidence(
                event_type="WARD_STATS",
                timestamp_ms=timeline_data["info"]["frames"][-1]["timestamp"],  # End of game
                formatted_timestamp=_format_timestamp(timeline_data["info"]["frames"][-1]["timestamp"]),
                details=ward_stats,
                player_context=None,
            )
        )
        return events

    elif dimension == "Objective Control":
        # Extract ELITE_MONSTER_KILL, BUILDING_KILL events
        # Focus on events where player was NOT present or contributed minimally
        pass

    elif dimension == "Combat":
        # Extract CHAMPION_KILL events involving player (as killer or victim)
        pass

    # ... other dimensions

    return []
```

---

#### 2.1.3 Helper Functions

```python
def _format_timestamp(timestamp_ms: int) -> str:
    """Convert milliseconds to MM:SS format.

    Example: 1456000 → "24:16"
    """
    total_seconds = timestamp_ms // 1000
    minutes = total_seconds // 60
    seconds = total_seconds % 60
    return f"{minutes}:{seconds:02d}"


def _get_location_name(position: dict[str, int]) -> str:
    """Convert (x, y) coordinates to human-readable location name.

    Summoner's Rift landmarks (approximate):
    - Baron pit: (9800, 4200)
    - Dragon pit: (9800, 4200) (Blue side) / (4200, 9800) (Red side)
    - River brush: (6000, 6000)
    - Jungle quadrants: Top/Bot/Blue/Red

    Returns:
        Location name in Chinese (e.g., "大龙坑", "河道草丛")
    """
    x, y = position["x"], position["y"]

    # Baron pit
    if 9000 <= x <= 10500 and 3500 <= y <= 4900:
        return "大龙坑"

    # Dragon pit
    if 9000 <= x <= 10500 and 9100 <= y <= 10600:
        return "小龙坑"

    # River
    if 5000 <= x <= 10000 and 5000 <= y <= 10000:
        return "河道草丛"

    # Default to quadrant
    if x < 7000 and y < 7000:
        return "下路野区"
    elif x < 7000 and y >= 7000:
        return "上路野区"
    # ... other quadrants

    return "未知位置"


def _calculate_ward_stats(
    timeline_data: dict[str, Any],
    player_puuid: str,
) -> dict[str, int]:
    """Calculate comprehensive ward statistics for player and support.

    Returns:
        Dictionary with keys:
            - player_total_wards: int
            - support_total_wards: int
            - player_avg_lifetime: int (seconds)
            - support_avg_lifetime: int (seconds)
    """
    # Identify support player (position 1 in team)
    support_participant_id = None  # Find from participants list

    # Count wards placed by player and support
    player_wards = []
    support_wards = []

    for frame in timeline_data["info"]["frames"]:
        for event in frame["events"]:
            if event["type"] == "WARD_PLACED":
                if event["creatorId"] == player_participant_id:
                    player_wards.append(event)
                elif event["creatorId"] == support_participant_id:
                    support_wards.append(event)

    # Calculate average lifetime (if wards were destroyed)
    player_avg_lifetime = _calculate_avg_ward_lifetime(player_wards, timeline_data)
    support_avg_lifetime = _calculate_avg_ward_lifetime(support_wards, timeline_data)

    return {
        "player_total_wards": len(player_wards),
        "support_total_wards": len(support_wards),
        "player_avg_lifetime": player_avg_lifetime,
        "support_avg_lifetime": support_avg_lifetime,
    }


def _identify_critical_impact_event(
    timeline_data: dict[str, Any],
    dimension: str,
    player_puuid: str,
) -> V21TimelineEvidence | None:
    """Identify the most critical event where player's weakness impacted outcome.

    Priority (highest impact first):
    1. Baron stolen due to lack of vision (Vision dimension)
    2. Dragon lost due to player absence (Objective Control)
    3. Team fight lost due to poor positioning (Combat/Teamplay)

    Returns:
        V21TimelineEvidence or None if no critical event found
    """
    if dimension == "Vision":
        # Find Baron/Dragon kills by enemy team
        for frame in timeline_data["info"]["frames"]:
            for event in frame["events"]:
                if event["type"] == "ELITE_MONSTER_KILL":
                    if event.get("monsterType") == "BARON_NASHOR":
                        # Check if team had vision in Baron area at this time
                        team_vision = _check_team_vision_in_area(
                            timeline_data, event["timestamp"], "baron"
                        )

                        if not team_vision:
                            # Get player's position at this time
                            player_pos = _get_player_position_at_time(
                                timeline_data, player_puuid, event["timestamp"]
                            )

                            return V21TimelineEvidence(
                                event_type="ELITE_MONSTER_KILL",
                                timestamp_ms=event["timestamp"],
                                formatted_timestamp=_format_timestamp(event["timestamp"]),
                                details={
                                    "monster_type": "BARON_NASHOR",
                                    "killer_team": "ENEMY",
                                    "team_vision_in_area": False,
                                },
                                player_context=f"You were farming {player_pos['location']} at {_format_timestamp(event['timestamp'] - 46000)}, missed Baron contest",
                            )

    # ... other dimensions

    return None
```

---

### Task 2.2: LLM Integration (Gemini Adapter Extension)

**File**: `src/adapters/gemini_llm.py` (MODIFY EXISTING)

#### 2.2.1 New Method: `generate_prescriptive_analysis_v21`

```python
from src.contracts.v21_prescriptive_analysis import (
    V21PrescriptiveAnalysisInput,
    V21PrescriptiveAnalysisReport,
)

class GeminiLLMAdapter:
    # ... existing methods ...

    async def generate_prescriptive_analysis_v21(
        self,
        input_data: V21PrescriptiveAnalysisInput,
    ) -> V21PrescriptiveAnalysisReport:
        """Generate V2.1 prescriptive analysis with structured suggestions.

        Uses coaching-framed prompt with strict JSON schema enforcement.

        Args:
            input_data: Prescriptive analysis input with timeline evidence

        Returns:
            V21PrescriptiveAnalysisReport with actionable suggestions

        Raises:
            ValueError: If LLM output fails Pydantic validation
            RuntimeError: If LLM API call fails
        """
        # Load prompt template
        prompt_template = self._load_prompt_template("v21_coaching_prescriptive.txt")

        # Format prompt with input data
        formatted_prompt = prompt_template.format(
            summoner_name=input_data.summoner_name,
            champion_name=input_data.champion_name,
            match_result=input_data.match_result,
            overall_score=input_data.overall_score,
            weak_dimensions_json=json.dumps(
                [dim.model_dump() for dim in input_data.weak_dimensions],
                ensure_ascii=False,
                indent=2,
            ),
        )

        # Call Gemini API with JSON Mode
        try:
            response = await self.client.generate_content(
                formatted_prompt,
                generation_config={
                    "response_mime_type": "application/json",
                    "response_schema": V21PrescriptiveAnalysisReport.model_json_schema(),
                    "temperature": 0.7,  # Balance creativity and consistency
                    "max_output_tokens": 2048,
                },
            )

            # Parse and validate JSON response
            report = V21PrescriptiveAnalysisReport.model_validate_json(response.text)

            # Populate performance metrics
            report.llm_input_tokens = response.usage_metadata.prompt_token_count
            report.llm_output_tokens = response.usage_metadata.candidates_token_count
            report.generation_latency_ms = int(response.latency_ms)  # If available

            return report

        except ValidationError as e:
            logger.error(
                f"LLM output failed Pydantic validation: {e}",
                extra={"llm_response": response.text if response else None},
            )
            raise ValueError(f"Invalid LLM output format: {e}")

        except Exception as e:
            logger.error(f"LLM API call failed: {e}", exc_info=True)
            raise RuntimeError(f"Failed to generate prescriptive analysis: {e}")


    def _load_prompt_template(self, filename: str) -> str:
        """Load prompt template from src/prompts/ directory."""
        prompt_path = Path(__file__).parent.parent / "prompts" / filename
        with open(prompt_path, "r", encoding="utf-8") as f:
            return f.read()
```

---

### Task 2.3: Celery Task Modification

**File**: `src/tasks/analysis_tasks.py` (MODIFY EXISTING)

#### 2.3.1 Add V2.1 Conditional Logic

```python
@celery_app.task(bind=True)
async def analyze_match_task(
    self,
    match_id: str,
    application_id: str,
    interaction_token: str,
    discord_user_id: str,
    enable_v21: bool = False,  # NEW: Feature flag for V2.1
):
    """Match analysis task with V2.1 prescriptive analysis support."""

    # ... STAGES 1-3: Data retrieval and scoring (unchanged)

    # ==================== STAGE 4: EVIDENCE EXTRACTION (V2.1 ONLY) ====================
    if enable_v21:
        logger.info(f"[STAGE 4] Extracting timeline evidence for V2.1 prescriptive analysis")

        # Identify weak dimensions (player_score < team_avg)
        weak_dimensions = []
        for dimension in ["Combat", "Economy", "Vision", "Objective Control", "Teamplay"]:
            player_score = score_data[f"{dimension.lower().replace(' ', '_')}_score"]
            team_avg = statistics.mean([
                player[f"{dimension.lower().replace(' ', '_')}_score"]
                for player in all_players_scores
            ])

            if player_score < team_avg:
                weak_dimensions.append({
                    "dimension": dimension,
                    "score": player_score,
                    "team_rank": _calculate_rank(player_score, all_players_scores, dimension),
                    "team_avg": team_avg,
                    "gap_from_avg": player_score - team_avg,
                })

        # Sort by gap_from_avg (worst first), limit to top 3
        weak_dimensions.sort(key=lambda x: x["gap_from_avg"])
        weak_dimensions = weak_dimensions[:3]

        # Extract timeline evidence
        v21_input = extract_prescriptive_evidence(
            timeline_data=match_timeline,  # From STAGE 2
            player_puuid=target_player_puuid,
            weak_dimensions=weak_dimensions,
            summoner_name=summoner_name,
            champion_name=champion_name,
            match_id=match_id,
            match_result=match_result,
            overall_score=score_data["overall_score"],
        )

        # ==================== STAGE 5: LLM GENERATION (V2.1) ====================
        logger.info(f"[STAGE 5] Generating V2.1 prescriptive analysis")

        v21_report = await gemini.generate_prescriptive_analysis_v21(v21_input)

        # Store report in database (extend match_analytics table)
        await db.store_v21_prescriptive_report(match_id, v21_report)

        # ==================== STAGE 6: DISCORD DELIVERY (V2.1) ====================
        # Publish via webhook (CLI 1 will render V2.1 embed)
        await webhook_adapter.publish_v21_prescriptive_analysis(
            application_id=application_id,
            interaction_token=interaction_token,
            report=v21_report,
        )

    else:
        # V2.0 flow (unchanged)
        pass
```

---

## 3. Database Schema Extensions

### 3.1 New Table: `v21_prescriptive_reports`

```sql
CREATE TABLE v21_prescriptive_reports (
    match_id VARCHAR(255) PRIMARY KEY REFERENCES match_analytics(match_id),

    -- Report metadata
    summoner_name VARCHAR(255) NOT NULL,
    champion_name VARCHAR(100) NOT NULL,
    match_result VARCHAR(10) NOT NULL CHECK (match_result IN ('victory', 'defeat')),

    -- Suggestions (JSON array)
    improvement_suggestions JSONB NOT NULL,  -- Array of V21ImprovementSuggestion
    total_weak_dimensions INT NOT NULL CHECK (total_weak_dimensions BETWEEN 1 AND 3),
    coaching_summary TEXT,

    -- Performance metrics
    llm_input_tokens INT,
    llm_output_tokens INT,
    generation_latency_ms INT,

    -- Metadata
    algorithm_version VARCHAR(10) DEFAULT 'v2.1',
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),

    -- Indexes
    INDEX idx_v21_summoner (summoner_name),
    INDEX idx_v21_created (created_at DESC)
);
```

### 3.2 New Table: `v21_suggestion_feedback`

```sql
CREATE TABLE v21_suggestion_feedback (
    id SERIAL PRIMARY KEY,

    match_id VARCHAR(255) NOT NULL REFERENCES v21_prescriptive_reports(match_id),
    suggestion_id VARCHAR(255) NOT NULL,  -- Format: "{dimension}_{timestamp_ms}"
    discord_user_id VARCHAR(255) NOT NULL,

    -- Feedback data
    is_actionable BOOLEAN NOT NULL,
    is_helpful BOOLEAN NOT NULL,
    feedback_comment TEXT,

    -- Metadata
    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
    interaction_id VARCHAR(255) UNIQUE,  -- Discord interaction ID (deduplication)

    -- Indexes
    INDEX idx_suggestion_feedback (match_id, suggestion_id),
    INDEX idx_user_feedback (discord_user_id),
    UNIQUE (match_id, suggestion_id, discord_user_id)  -- Prevent duplicate feedback
);
```

---

## 4. Testing Plan

### 4.1 Unit Tests

**File**: `tests/unit/test_timeline_evidence_extractor.py` (NEW)

```python
import pytest
from src.core.services.timeline_evidence_extractor import (
    extract_prescriptive_evidence,
    _format_timestamp,
    _get_location_name,
)

def test_format_timestamp():
    """Test timestamp conversion from milliseconds to MM:SS."""
    assert _format_timestamp(1456000) == "24:16"
    assert _format_timestamp(734000) == "12:14"
    assert _format_timestamp(60000) == "1:00"


def test_get_location_name():
    """Test coordinate to location name conversion."""
    assert _get_location_name({"x": 9800, "y": 4200}) == "大龙坑"
    assert _get_location_name({"x": 9800, "y": 9800}) == "小龙坑"
    # ... more test cases


@pytest.mark.asyncio
async def test_extract_prescriptive_evidence_vision():
    """Test evidence extraction for Vision dimension."""
    # Mock timeline data with WARD_PLACED events
    mock_timeline = {...}  # Load from test fixtures
    mock_weak_dimensions = [
        {
            "dimension": "Vision",
            "score": 62.4,
            "team_rank": 4,
            "team_avg": 75.3,
            "gap_from_avg": -12.9,
        }
    ]

    result = extract_prescriptive_evidence(
        timeline_data=mock_timeline,
        player_puuid="test_puuid",
        weak_dimensions=mock_weak_dimensions,
        summoner_name="TestPlayer",
        champion_name="Jinx",
        match_id="NA1_12345",
        match_result="defeat",
        overall_score=77.8,
    )

    # Assertions
    assert len(result.weak_dimensions) == 1
    assert result.weak_dimensions[0].dimension == "Vision"
    assert len(result.weak_dimensions[0].evidence) > 0
    assert result.weak_dimensions[0].critical_impact_event is not None
```

### 4.2 Integration Tests

**File**: `tests/integration/test_v21_prescriptive_flow.py` (NEW)

```python
@pytest.mark.asyncio
async def test_v21_end_to_end_flow():
    """Test complete V2.1 flow from timeline extraction to LLM generation."""
    # 1. Load real Match-V5 Timeline data from fixture
    timeline_data = load_fixture("match_timeline_na1_5387390374.json")

    # 2. Extract evidence
    v21_input = extract_prescriptive_evidence(...)

    # 3. Call LLM (use mock in CI, real in local testing)
    gemini = GeminiLLMAdapter()
    v21_report = await gemini.generate_prescriptive_analysis_v21(v21_input)

    # 4. Validate output
    assert len(v21_report.improvement_suggestions) > 0
    assert v21_report.improvement_suggestions[0].priority in ["critical", "high", "medium"]
    assert v21_report.llm_input_tokens > 0

    # 5. Check Pydantic validation
    assert v21_report.model_validate(v21_report.model_dump())
```

---

## 5. Rollout Plan

### Phase 1: Development & Testing (Week 1)

**Days 1-3**: Timeline Evidence Extraction
- Implement `extract_prescriptive_evidence` function
- Unit test all helper functions (_format_timestamp, _get_location_name, etc.)
- Integration test with real Match-V5 Timeline data (5 test matches)

**Days 4-5**: LLM Integration
- Implement `generate_prescriptive_analysis_v21` in Gemini adapter
- Test prompt template with 10 sample inputs
- Validate JSON schema enforcement (catch hallucination failures)

**Days 6-7**: Celery Task Modification
- Add `enable_v21` feature flag to `analyze_match_task`
- Integration test complete flow (data → evidence → LLM → storage)
- Performance benchmarking (latency, token cost)

---

### Phase 2: Staged Rollout (Week 2)

**Days 8-9**: Internal Testing
- Enable V2.1 for CLI 4's personal Discord account only
- Manually review 20-30 generated suggestions for quality
- Collect qualitative feedback from CLI 4

**Day 10**: Limited Production Rollout
- Enable V2.1 for **5% of users** (`enable_v21=True` for 5% traffic)
- Monitor for errors, JSON parsing failures, user complaints
- Collect 50+ suggestions for manual quality review

**Days 11-14**: Gradual Expansion
- If Phase 2 quality check passes → Increase to **20% of users**
- Continue monitoring Grafana metrics (latency, cost, failure rate)
- Prepare for V2.1 vs V2.0 A/B test (CLI 4 will design success criteria)

---

## 6. Riot Policy Compliance Checklist

**CRITICAL**: V2.1 must comply with Riot Games' game integrity policy.

### ✅ Allowed (Post-Game Training)

- [x] Analyze player's past decisions based on timeline data
- [x] Suggest improvements for future games ("In similar situations, consider...")
- [x] Compare player's performance with teammates (post-game)
- [x] Provide educational coaching framed as training tool

### ❌ Prohibited (Real-Time Advantage)

- [ ] Track enemy ability cooldowns in real-time
- [ ] Provide information not visible in-game UI
- [ ] Automate in-game decision-making
- [ ] Give competitive advantage beyond human observation

**Validation**: CLI 4 has manually reviewed V2.1 prompt template and confirmed compliance. All suggestions are **post-game analysis only**.

---

## 7. Expected Performance Metrics

### 7.1 Token Cost Estimate

**V2.1 Input** (per analysis):
- Prompt template: ~400 tokens
- Player metadata: ~50 tokens
- Weak dimensions (3 dimensions × 200 tokens evidence): ~600 tokens
- **Total Input**: **~1,050 tokens** (+31% vs. V2.0's 800 tokens)

**V2.1 Output** (per analysis):
- 3-5 suggestions × 150 tokens each: ~600 tokens
- Coaching summary: ~50 tokens
- **Total Output**: **~650 tokens** (+200% vs. V2.0's 200 tokens)

**Cost Impact** (Gemini Pro pricing: $0.25/M input, $1.00/M output):
- V2.1 cost: (1,050 × $0.00025 + 650 × $0.001) / 1000 = **$0.000913 per analysis**
- V2.0 cost: **$0.000403 per analysis**
- **Increase**: **+126%** (acceptable for high-value feature)

---

### 7.2 Latency Estimate

**V2.1 Additional Latency** (per analysis):
- Timeline evidence extraction: **+500ms** (parsing timeline data)
- LLM generation: **+1,200ms** (longer prompt, more output tokens)
- **Total Additional Latency**: **~1,700ms**

**Projected P95 Latency**:
- V2.0: 12,800ms
- V2.1: 12,800ms + 1,700ms = **14,500ms** (+13.3%)
- **Status**: Within acceptable threshold (<20% increase)

---

## 8. Success Criteria for V2.1 Rollout

**Defined by CLI 4** (based on V2.0 A/B test experience):

| Criterion | Threshold | Measurement |
|-----------|-----------|-------------|
| **User Actionability Rating** | ≥ 75% "helpful" | V21SuggestionFeedback.is_helpful = true |
| **JSON Parsing Failure Rate** | < 3% | LLM output validation failures |
| **Token Cost Increase** | < 150% | Actual cost vs. V2.0 baseline |
| **P95 Latency Increase** | < 20% | Grafana metric vs. V2.0 baseline |
| **User Complaints** | < 5 per week | Discord support channel |

**Decision Matrix**:
- If all criteria met → Promote V2.1 to 100% (replace V2.0)
- If actionability < 75% → Refine prompt template, iterate
- If cost > 150% → Optimize evidence extraction (reduce input tokens)

---

## 9. Handoff Checklist

**CLI 4 → CLI 2 Deliverables** ✅:

- [x] Pydantic data contracts (`src/contracts/v21_prescriptive_analysis.py`)
- [x] Prompt template (`src/prompts/v21_coaching_prescriptive.txt`)
- [x] This engineering integration guide (implementation spec)
- [x] Example data (`EXAMPLE_V21_INPUT`, `EXAMPLE_V21_OUTPUT`)
- [x] Database schema (`v21_prescriptive_reports`, `v21_suggestion_feedback`)
- [x] Testing plan (unit + integration tests)
- [x] Rollout plan (2-week timeline)
- [x] Success criteria (actionability, cost, latency)

**CLI 2 Acceptance Criteria**:

- [ ] All unit tests pass (timeline evidence extraction)
- [ ] Integration test with real Match-V5 data succeeds
- [ ] LLM generates valid JSON output (5/5 test cases)
- [ ] Internal testing (20 analyses) shows ≥ 80% actionability
- [ ] Performance benchmarks meet targets (latency < 15s P95)

---

## 10. Support & Escalation

**Questions during implementation?**

1. **Pydantic Schema Issues**: Check `src/contracts/v21_prescriptive_analysis.py` comments
2. **Prompt Template Questions**: Refer to `v21_coaching_prescriptive.txt` inline documentation
3. **Timeline API Confusion**: Check Match-V5 Timeline docs: https://developer.riotgames.com/apis#match-v5
4. **Complex Decisions**: Escalate to CLI 4 (The Lab) for architectural guidance

**Contact**: CLI 4 (The Lab) via project Discord channel

---

**Document Status**: ✅ **Ready for CLI 2 Implementation**
**Next Milestone**: CLI 2 completes Phase 1 development (Week 1)
**Owner**: CLI 4 (The Lab) → CLI 2 (Backend Engineer)
**Date**: 2025-10-06
