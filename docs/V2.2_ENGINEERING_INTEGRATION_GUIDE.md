# V2.2 Personalization Feature - Engineering Integration Guide

**Author**: CLI 4 (The Lab)
**Date**: 2025-10-06
**Target Audience**: CLI 2 (Backend), CLI 1 (Frontend)
**Status**: ✅ Production Ready
**Research Foundation**: `notebooks/v2.2_personalization.ipynb`

---

## 📋 Table of Contents

1. [Feature Overview](#feature-overview)
2. [Architecture Diagram](#architecture-diagram)
3. [Data Flow](#data-flow)
4. [Integration Tasks](#integration-tasks)
5. [Testing Strategy](#testing-strategy)
6. [Rollout Plan](#rollout-plan)
7. [Success Criteria](#success-criteria)
8. [Handoff Checklist](#handoff-checklist)

---

## Feature Overview

### V2.2 Core Value Proposition

**Personalization** enables "千人千面" (one thousand faces) customization of prescriptive analysis based on:

1. **User Preferences** (explicit via `/settings`):
   - Analysis tone: competitive vs casual
   - Primary role preference

2. **Performance Trends** (inferred from history):
   - Persistent weak dimensions (e.g., Vision low in 75% of matches)
   - Average scores across dimensions

3. **Player Classification** (auto-detected):
   - Skill level: beginner / intermediate / advanced
   - Player type: casual / competitive

### Key Benefits

- **+15-20pp improvement** in "Helpfulness" ratings (hypothesis)
- **Role-specific suggestions**: ADC vs Jungle receive different Vision advice
- **Tone customization**: Competitive players get concise, data-heavy feedback; casual players get friendly, explanatory guidance
- **Persistent weakness prioritization**: Users with ongoing Vision issues see Vision suggestions first, even if other dimensions had larger gaps in current match

---

## Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────────┐
│                        V2.2 Personalization Flow                    │
└─────────────────────────────────────────────────────────────────────┘

  Discord User
      │
      │ /settings command (set tone, role)
      ▼
┌──────────────┐
│   CLI 1      │ ──────────► /user_preferences endpoint
│  (Frontend)  │
└──────────────┘
      │
      │ Store preferences
      ▼
┌──────────────────────────────────────────────────────────────────┐
│  CLI 2 (Backend) - UserProfileService                            │
│                                                                  │
│  1. Receive /jiangli command                                    │
│  2. Load/Create V22UserProfile for user                         │
│  3. Pass profile to PersonalizationService                      │
│  4. PersonalizationService:                                     │
│     ├─ Select prompt template (competitive/casual)              │
│     ├─ Generate user context (persistent weakness, role, etc.)  │
│     └─ Inject context into V2.1 prompt                          │
│  5. Call Gemini LLM with personalized prompt                    │
│  6. Return V21PrescriptiveAnalysisReport                        │
│  7. Update user profile with new match data                     │
└──────────────────────────────────────────────────────────────────┘
      │
      │ V21PrescriptiveAnalysisReport (same as V2.1)
      ▼
┌──────────────┐
│   CLI 1      │ ──────────► Render suggestions (no change from V2.1)
│  (Frontend)  │
└──────────────┘
```

---

## Data Flow

### Phase 1: User Preference Configuration (CLI 1 → CLI 2)

**CLI 1 Task**: Implement `/settings` command

```python
# CLI 1: src/contracts/discord_interactions.py

class UserPreferencesModal(discord.ui.Modal):
    """Modal for user to configure V2.2 preferences."""

    preferred_tone = discord.ui.TextInput(
        label="分析语气 (Analysis Tone)",
        placeholder="competitive (竞技型) 或 casual (休闲型)",
        required=False,
        default="casual"
    )

    preferred_role = discord.ui.TextInput(
        label="主要位置 (Primary Role)",
        placeholder="Top, Jungle, Mid, ADC, Support, 或 Fill",
        required=False,
    )

    async def on_submit(self, interaction: discord.Interaction):
        # Call CLI 2 API to save preferences
        await backend_api.save_user_preferences(
            discord_user_id=interaction.user.id,
            preferred_tone=self.preferred_tone.value,
            preferred_role=self.preferred_role.value,
        )
        await interaction.response.send_message(
            "✅ 偏好设置已保存！下次分析将根据你的偏好定制。",
            ephemeral=True
        )
```

**CLI 2 Task**: Implement `/user_preferences` API endpoint

```python
# CLI 2: src/api/user_profile.py

from fastapi import APIRouter, Depends
from src.contracts.v22_user_profile import V22UserPreferences

router = APIRouter(prefix="/api/v2.2", tags=["user_profile"])

@router.post("/user_preferences")
async def save_user_preferences(
    discord_user_id: str,
    preferences: V22UserPreferences,
    db: Database = Depends(get_database),
):
    """Save or update user's explicit preferences from /settings command."""

    # Upsert preferences in user_profiles table
    await db.upsert_user_preferences(
        discord_user_id=discord_user_id,
        preferences=preferences.model_dump()
    )

    return {"status": "success", "message": "Preferences saved"}
```

---

### Phase 2: Profile Building & Incremental Updates (CLI 2)

**Task 2.1**: Implement `UserProfileService`

```python
# CLI 2: src/core/services/user_profile_service.py

from datetime import datetime, UTC
from src.contracts.v22_user_profile import (
    V22UserProfile,
    V22PerformanceTrends,
    V22ChampionProfile,
    V22UserClassification,
    V22ProfileUpdateEvent,
)


class UserProfileService:
    """Service for building and maintaining V2.2 user profiles."""

    def __init__(self, database, riot_api_adapter):
        self.db = database
        self.riot_api = riot_api_adapter

    async def get_or_create_profile(
        self,
        discord_user_id: str,
        puuid: str,
    ) -> V22UserProfile:
        """Load existing profile or create new one with defaults.

        Returns:
            V22UserProfile with latest data
        """
        # Try to load from database
        existing_profile = await self.db.load_user_profile(discord_user_id)

        if existing_profile:
            return V22UserProfile.model_validate(existing_profile)

        # Create new profile with defaults
        new_profile = V22UserProfile(
            discord_user_id=discord_user_id,
            puuid=puuid,
            total_matches_analyzed=0,
            last_updated=datetime.now(UTC).isoformat(),
        )

        # Save to database
        await self.db.save_user_profile(
            discord_user_id=discord_user_id,
            profile_data=new_profile.model_dump()
        )

        return new_profile

    async def update_profile_after_match(
        self,
        update_event: V22ProfileUpdateEvent,
    ) -> V22UserProfile:
        """Incrementally update user profile after new match analysis.

        This is called from analyze_team_task after V2.1 analysis completes.
        """
        # Load current profile
        profile = await self.get_or_create_profile(
            discord_user_id=update_event.discord_user_id,
            puuid=update_event.puuid,
        )

        # Update champion profile
        profile = await self._update_champion_profile(profile, update_event)

        # Update performance trends (requires last 20 matches)
        profile = await self._update_performance_trends(profile, update_event)

        # Update classification (if ranked tier changed)
        profile = await self._update_classification(profile)

        # Update metadata
        profile.total_matches_analyzed += 1
        profile.last_updated = datetime.now(UTC).isoformat()

        # Persist updated profile
        await self.db.save_user_profile(
            discord_user_id=profile.discord_user_id,
            profile_data=profile.model_dump()
        )

        return profile

    async def _update_champion_profile(
        self,
        profile: V22UserProfile,
        update_event: V22ProfileUpdateEvent,
    ) -> V22UserProfile:
        """Update champion play counts and role distribution."""

        # Update champion counts
        champion = update_event.played_champion
        current_count = profile.champion_profile.champion_play_counts.get(champion, 0)
        profile.champion_profile.champion_play_counts[champion] = current_count + 1

        # Update role counts
        role = update_event.played_role
        current_role_count = profile.champion_profile.role_distribution.get(role, 0)
        profile.champion_profile.role_distribution[role] = current_role_count + 1

        # Recalculate top 3 champions
        sorted_champions = sorted(
            profile.champion_profile.champion_play_counts.items(),
            key=lambda x: x[1],
            reverse=True
        )
        profile.champion_profile.top_3_champions = [
            champ for champ, _ in sorted_champions[:3]
        ]

        # Infer primary role (≥40% threshold)
        total_matches = sum(profile.champion_profile.role_distribution.values())
        if total_matches >= 5:  # Minimum 5 matches for inference
            for role, count in profile.champion_profile.role_distribution.items():
                if count / total_matches >= 0.4:
                    profile.champion_profile.inferred_primary_role = role
                    break
            else:
                profile.champion_profile.inferred_primary_role = "Fill"

        return profile

    async def _update_performance_trends(
        self,
        profile: V22UserProfile,
        update_event: V22ProfileUpdateEvent,
    ) -> V22UserProfile:
        """Update performance trends from last 20 matches."""

        # Fetch last 20 match analysis results from database
        recent_matches = await self.db.get_recent_match_analyses(
            puuid=profile.puuid,
            limit=20
        )

        if len(recent_matches) < 5:
            # Insufficient data for trends
            profile.performance_trends = None
            return profile

        # Calculate average scores
        avg_scores = {
            "Combat": sum(m["dimension_scores"]["Combat"] for m in recent_matches) / len(recent_matches),
            "Economy": sum(m["dimension_scores"]["Economy"] for m in recent_matches) / len(recent_matches),
            "Vision": sum(m["dimension_scores"]["Vision"] for m in recent_matches) / len(recent_matches),
            "Objective Control": sum(m["dimension_scores"]["Objective Control"] for m in recent_matches) / len(recent_matches),
            "Teamplay": sum(m["dimension_scores"]["Teamplay"] for m in recent_matches) / len(recent_matches),
        }

        # Identify persistent weak dimension (below team avg in ≥70% of matches)
        dimension_below_avg_counts = {dim: 0 for dim in avg_scores.keys()}

        for match in recent_matches:
            for dim in avg_scores.keys():
                if match["dimension_scores"][dim] < match["team_avg_scores"][dim]:
                    dimension_below_avg_counts[dim] += 1

        persistent_weak_dim = None
        weak_dim_frequency = None

        for dim, below_count in dimension_below_avg_counts.items():
            frequency = below_count / len(recent_matches)
            if frequency >= 0.7:
                persistent_weak_dim = dim
                weak_dim_frequency = frequency
                break  # Only track the most frequent weakness

        # Calculate win rate
        wins = sum(1 for m in recent_matches if m["match_result"] == "victory")
        recent_win_rate = wins / len(recent_matches)

        # Update profile
        profile.performance_trends = V22PerformanceTrends(
            avg_combat_score=avg_scores["Combat"],
            avg_economy_score=avg_scores["Economy"],
            avg_vision_score=avg_scores["Vision"],
            avg_objective_control_score=avg_scores["Objective Control"],
            avg_teamplay_score=avg_scores["Teamplay"],
            persistent_weak_dimension=persistent_weak_dim,
            weak_dimension_frequency=weak_dim_frequency,
            recent_win_rate=recent_win_rate,
        )

        return profile

    async def _update_classification(
        self,
        profile: V22UserProfile,
    ) -> V22UserProfile:
        """Update user classification (skill level, player type)."""

        # Fetch ranked tier from Riot API
        try:
            ranked_data = await self.riot_api.get_ranked_stats(profile.puuid)
            ranked_tier = ranked_data.get("tier", "UNRANKED")
        except Exception:
            ranked_tier = None

        # Infer skill level from ranked tier
        if ranked_tier in ["IRON", "BRONZE"]:
            skill_level = "beginner"
        elif ranked_tier in ["SILVER", "GOLD"]:
            skill_level = "intermediate"
        elif ranked_tier:  # PLATINUM, DIAMOND, MASTER, etc.
            skill_level = "advanced"
        else:
            skill_level = "intermediate"  # Default

        # Calculate matches per week (from last 28 days)
        matches_last_28_days = await self.db.get_match_count_in_period(
            puuid=profile.puuid,
            days=28
        )
        matches_per_week = matches_last_28_days / 4

        # Infer player type
        is_competitive = (
            matches_per_week >= 5
            and ranked_tier in ["GOLD", "PLATINUM", "DIAMOND", "MASTER", "GRANDMASTER", "CHALLENGER"]
        )
        player_type = "competitive" if is_competitive else "casual"

        # Update profile
        profile.classification = V22UserClassification(
            skill_level=skill_level,
            player_type=player_type,
            ranked_tier=ranked_tier,
            matches_per_week=matches_per_week,
        )

        return profile
```

---

### Phase 3: Personalized Analysis Generation (CLI 2 Integration)

**Task 3.1**: Modify `analyze_team_task` to use `PersonalizationService`

```python
# CLI 2: src/tasks/team_analysis_task.py

from src.core.services.personalization_service import PersonalizationService
from src.core.services.user_profile_service import UserProfileService
from src.contracts.v22_user_profile import V22ProfileUpdateEvent

@celery_app.task(name="analyze_team")
async def analyze_team_task(
    match_id: str,
    puuid: str,
    discord_user_id: str,
    enable_v21: bool = True,
    enable_v22: bool = False,  # Feature flag for V2.2
):
    """Enhanced analyze_team_task with V2.2 personalization support."""

    # ... (existing V1 + V2.1 analysis logic)

    if enable_v22:
        # --- V2.2 PERSONALIZATION FLOW ---

        # Step 1: Load or create user profile
        user_profile_service = UserProfileService(database, riot_api_adapter)
        user_profile = await user_profile_service.get_or_create_profile(
            discord_user_id=discord_user_id,
            puuid=puuid,
        )

        # Step 2: Generate personalized V2.1 analysis
        personalization_service = PersonalizationService(
            prompt_templates_dir=Path("src/prompts")
        )

        report = await personalization_service.generate_personalized_analysis(
            user_profile=user_profile,
            analysis_input=v21_input,
            llm_adapter=gemini_adapter,
        )

        # Step 3: Update user profile after match
        update_event = V22ProfileUpdateEvent(
            discord_user_id=discord_user_id,
            puuid=puuid,
            match_id=match_id,
            match_result=v21_input.match_result,
            played_role=player_role,
            played_champion=v21_input.champion_name,
            dimension_scores={
                "Combat": combat_score,
                "Economy": economy_score,
                "Vision": vision_score,
                "Objective Control": obj_control_score,
                "Teamplay": teamplay_score,
            },
            team_avg_scores=team_avg_scores,
            analyzed_at=datetime.now(UTC).isoformat(),
        )

        await user_profile_service.update_profile_after_match(update_event)

    elif enable_v21:
        # Baseline V2.1 analysis (no personalization)
        report = await gemini_adapter.generate_prescriptive_analysis_v21(
            input_data=v21_input
        )

    # ... (store report, return to CLI 1)
```

**Task 3.2**: Add `generate_prescriptive_analysis_v22` method to `GeminiLLMAdapter`

```python
# CLI 2: src/adapters/gemini_llm.py

class GeminiLLMAdapter:
    """Enhanced Gemini adapter with V2.2 personalization support."""

    async def generate_prescriptive_analysis_v22(
        self,
        input_data: V21PrescriptiveAnalysisInput,
        prompt_template: str,  # From PersonalizationService
        user_context: str,     # From PersonalizationService
    ) -> V21PrescriptiveAnalysisReport:
        """Generate V2.2 personalized prescriptive analysis.

        Args:
            input_data: V2.1 analysis input (weak dimensions + evidence)
            prompt_template: Selected prompt template (competitive/casual)
            user_context: User profile context for injection

        Returns:
            V21PrescriptiveAnalysisReport (same output contract as V2.1)
        """

        # Format prompt with user context
        formatted_prompt = prompt_template.format(
            user_profile_context=user_context,
            summoner_name=input_data.summoner_name,
            champion_name=input_data.champion_name,
            match_result=input_data.match_result,
            overall_score=input_data.overall_score,
            weak_dimensions_json=json.dumps(
                [dim.model_dump() for dim in input_data.weak_dimensions],
                ensure_ascii=False,
                indent=2
            ),
        )

        # Call Gemini with JSON Mode (same as V2.1)
        response = await self.client.generate_content(
            prompt=formatted_prompt,
            response_mime_type="application/json",
            response_schema=V21PrescriptiveAnalysisReport,
        )

        # Parse and validate
        report = V21PrescriptiveAnalysisReport.model_validate_json(
            response.text
        )

        # Add metadata
        report.llm_input_tokens = response.usage_metadata.input_tokens
        report.llm_output_tokens = response.usage_metadata.output_tokens
        report.algorithm_version = "v2.2"  # Track V2.2 usage

        return report
```

---

## Integration Tasks

### CLI 1 (Frontend) Tasks

| Task ID | Description | Estimated Effort | Dependencies |
|---------|-------------|------------------|--------------|
| CLI1-V2.2-T1 | Implement `/settings` command with modal for preference configuration | 2 days | None |
| CLI1-V2.2-T2 | Add API client method for `POST /api/v2.2/user_preferences` | 0.5 days | CLI2-V2.2-T1 |
| CLI1-V2.2-T3 | Update `/help` command documentation to mention `/settings` | 0.5 days | CLI1-V2.2-T1 |

**Total CLI 1 Effort**: 3 days

---

### CLI 2 (Backend) Tasks

| Task ID | Description | Estimated Effort | Dependencies |
|---------|-------------|------------------|--------------|
| CLI2-V2.2-T1 | Implement `POST /api/v2.2/user_preferences` endpoint | 1 day | None |
| CLI2-V2.2-T2 | Implement `UserProfileService` (profile building & incremental updates) | 4 days | None |
| CLI2-V2.2-T3 | Add `generate_prescriptive_analysis_v22` method to `GeminiLLMAdapter` | 1 day | None |
| CLI2-V2.2-T4 | Modify `analyze_team_task` to integrate `PersonalizationService` | 2 days | CLI2-V2.2-T2, CLI2-V2.2-T3 |
| CLI2-V2.2-T5 | Database migration: Add `user_profiles` table | 1 day | None |
| CLI2-V2.2-T6 | Deploy V2.2 prompt templates to production | 0.5 days | None |

**Total CLI 2 Effort**: 9.5 days (~2 weeks)

---

## Testing Strategy

### Unit Tests

**Test `PersonalizationService.select_prompt_template`**:
```python
def test_select_prompt_template_explicit_preference():
    """User explicitly set 'competitive' in /settings."""
    profile = V22UserProfile(
        preferences=V22UserPreferences(preferred_analysis_tone="competitive"),
        classification=V22UserClassification(player_type="casual"),
        # ... other fields
    )

    service = PersonalizationService(prompt_templates_dir=Path("src/prompts"))
    template = service.select_prompt_template(profile)

    assert template == "v22_coaching_competitive.txt"


def test_select_prompt_template_inferred_player_type():
    """User didn't set preference, infer from player_type."""
    profile = V22UserProfile(
        preferences=V22UserPreferences(preferred_analysis_tone=None),
        classification=V22UserClassification(player_type="casual"),
        # ... other fields
    )

    service = PersonalizationService(prompt_templates_dir=Path("src/prompts"))
    template = service.select_prompt_template(profile)

    assert template == "v22_coaching_casual.txt"
```

**Test `PersonalizationService.generate_user_context`**:
```python
def test_generate_user_context_with_persistent_weakness():
    """Generate context mentioning persistent Vision weakness."""
    profile = V22UserProfile(
        champion_profile=V22ChampionProfile(inferred_primary_role="Jungle"),
        performance_trends=V22PerformanceTrends(
            avg_vision_score=45.2,
            persistent_weak_dimension="Vision",
            weak_dimension_frequency=0.75,
            # ... other fields
        ),
        # ... other fields
    )

    service = PersonalizationService(prompt_templates_dir=Path("src/prompts"))
    context = service.generate_user_context(profile, mock_v21_input)

    assert "Jungle 位置玩家" in context
    assert "Vision 维度得分持续偏低" in context
    assert "平均 45.2 分" in context
    assert "75% 的比赛中低于队伍平均水平" in context
```

**Test `UserProfileService._update_performance_trends`**:
```python
@pytest.mark.asyncio
async def test_identify_persistent_weak_dimension():
    """Identify Vision as persistent weakness (below team avg in 15/20 matches)."""

    # Mock 20 matches where Vision is below team avg in 15 matches (75%)
    mock_matches = [
        {
            "dimension_scores": {"Vision": 40, "Combat": 80, ...},
            "team_avg_scores": {"Vision": 65, "Combat": 75, ...},
            "match_result": "defeat",
        }
        for _ in range(15)  # Vision below avg
    ] + [
        {
            "dimension_scores": {"Vision": 70, "Combat": 80, ...},
            "team_avg_scores": {"Vision": 65, "Combat": 75, ...},
            "match_result": "victory",
        }
        for _ in range(5)  # Vision above avg
    ]

    service = UserProfileService(mock_database, mock_riot_api)
    mock_database.get_recent_match_analyses.return_value = mock_matches

    profile = V22UserProfile(puuid="test-puuid", ...)
    updated_profile = await service._update_performance_trends(profile, mock_event)

    assert updated_profile.performance_trends.persistent_weak_dimension == "Vision"
    assert updated_profile.performance_trends.weak_dimension_frequency == 0.75
```

---

### Integration Tests

**Test End-to-End V2.2 Flow**:
```python
@pytest.mark.asyncio
async def test_e2e_v22_personalized_analysis():
    """E2E test: /jiangli command with V2.2 personalization enabled."""

    # Setup: Create user with persistent Vision weakness
    user_profile = await create_test_user_profile(
        discord_user_id="test_user",
        puuid="test_puuid",
        persistent_weak_dimension="Vision",
        preferred_tone="competitive",
    )

    # Execute: Trigger analysis with enable_v22=True
    result = await analyze_team_task(
        match_id="NA1_12345",
        puuid="test_puuid",
        discord_user_id="test_user",
        enable_v22=True,
    )

    # Verify: Report should mention persistent Vision weakness
    report = result["v21_report"]
    assert report.algorithm_version == "v2.2"

    # Find Vision suggestion (should be prioritized)
    vision_suggestions = [
        s for s in report.improvement_suggestions if s.dimension == "Vision"
    ]
    assert len(vision_suggestions) >= 1

    # Verify issue mentions historical trend
    vision_issue = vision_suggestions[0].issue_identified
    assert "最近20场比赛" in vision_issue or "持续" in vision_issue
```

---

## Rollout Plan

### Phase 1: Internal Testing (Week 1)

- **Target**: CLI development team only
- **Configuration**: `enable_v22=True` for specific Discord user IDs
- **Validation**:
  - User profiles are correctly built and updated
  - Prompt templates are selected correctly (competitive vs casual)
  - User context injection works as expected

### Phase 2: Opt-In Beta (Week 2-3)

- **Target**: 20 volunteers from Discord community
- **Configuration**: Add beta testers to allowlist
- **Validation**:
  - Collect feedback on tone appropriateness
  - Monitor helpfulness ratings (should improve by ≥5pp)
  - Check for any LLM hallucination or incorrect context injection

### Phase 3: A/B Test (Week 4-5)

- **Target**: 20% of all users (hash-based cohort)
- **Configuration**: A/B test framework (`algorithm_version` = "v2.1" vs "v2.2")
- **Validation**:
  - Compare helpfulness ratings (V2.2 should beat V2.1 by ≥5pp)
  - Compare actionability ratings (V2.2 should maintain ≥75%)
  - Monitor token cost increase (target: ≤30% increase)

### Phase 4: Full Rollout (Week 6)

- **Target**: 100% of users (if A/B test succeeds)
- **Configuration**: Set `enable_v22=True` as default

---

## Success Criteria

| Metric | V2.1 Baseline | V2.2 Target | Measurement Method |
|--------|---------------|-------------|---------------------|
| **Helpfulness Rate** | 72% | ≥77% (+5pp) | `V21SuggestionFeedback.is_helpful` |
| **Actionability Rate** | 75% | ≥75% (maintain) | `V21SuggestionFeedback.is_actionable` |
| **Token Cost Increase** | Baseline | ≤30% | `llm_input_tokens + llm_output_tokens` |
| **User Engagement** | Baseline | +15% feedback comment length | Avg `feedback_comment` length |
| **Profile Coverage** | N/A | ≥80% users have profiles | Users with `total_matches_analyzed ≥ 5` |

### Decision Matrix

| Helpfulness | Actionability | Token Cost | Decision |
|-------------|---------------|------------|----------|
| +5pp | ≥75% | ≤30% | ✅ PROMOTE V2.2 TO 100% |
| +3-5pp | ≥75% | ≤30% | 🟡 CONDITIONAL PROMOTION (monitor 2 more weeks) |
| <+3pp | <75% | >30% | ❌ ROLLBACK TO V2.1 |
| +5pp | ≥75% | >30% | 🟡 OPTIMIZE PROMPTS (reduce verbosity) |

---

## Handoff Checklist

### CLI 4 (The Lab) Deliverables ✅

- [x] `src/contracts/v22_user_profile.py` - Complete data contracts
- [x] `src/core/services/personalization_service.py` - PersonalizationService implementation
- [x] `src/prompts/v22_coaching_competitive.txt` - Competitive prompt template
- [x] `src/prompts/v22_coaching_casual.txt` - Casual prompt template
- [x] `docs/V2.2_ENGINEERING_INTEGRATION_GUIDE.md` - This document

### CLI 2 (Backend) Acceptance Criteria

- [ ] `UserProfileService` implemented and tested
- [ ] `POST /api/v2.2/user_preferences` endpoint deployed
- [ ] `analyze_team_task` modified to support `enable_v22` flag
- [ ] `GeminiLLMAdapter.generate_prescriptive_analysis_v22` method added
- [ ] Database migration completed (`user_profiles` table created)
- [ ] Prompt templates deployed to production environment
- [ ] Unit tests pass (≥80% coverage for new code)
- [ ] Integration tests pass (E2E V2.2 flow validated)

### CLI 1 (Frontend) Acceptance Criteria

- [ ] `/settings` command implemented with preference modal
- [ ] API integration for saving user preferences completed
- [ ] `/help` documentation updated to mention `/settings`

### CLI 3 (DevOps) Monitoring Setup

- [ ] Add metric: `v22_profile_build_latency_ms` (target: <500ms)
- [ ] Add metric: `v22_prompt_injection_success_rate` (target: 100%)
- [ ] Add alert: `v22_token_cost_spike` (if >50% increase over V2.1)
- [ ] Add dashboard: V2.2 vs V2.1 A/B test comparison

---

## Estimated Timeline

| Phase | Duration | Start Date | End Date |
|-------|----------|------------|----------|
| CLI 2 Implementation | 2 weeks | 2025-10-07 | 2025-10-18 |
| CLI 1 Implementation | 3 days | 2025-10-07 | 2025-10-09 |
| Internal Testing | 1 week | 2025-10-21 | 2025-10-25 |
| Opt-In Beta | 2 weeks | 2025-10-28 | 2025-11-08 |
| A/B Test | 2 weeks | 2025-11-11 | 2025-11-22 |
| Full Rollout | 1 week | 2025-11-25 | 2025-11-29 |

**Total Time to Production**: ~8 weeks from handoff

---

## Questions & Support

**For CLI 2 Implementation Questions**: Contact CLI 4 (The Lab) with tag `[V2.2-Backend]`
**For CLI 1 Implementation Questions**: Contact CLI 4 (The Lab) with tag `[V2.2-Frontend]`
**For Research Context**: Review `notebooks/v2.2_personalization.ipynb`

---

**Engineering Handoff Status**: ✅ Ready for CLI 2 & CLI 1 Acceptance
**Confidence Level**: High (comprehensive research + production-ready code)
**Estimated V2.2 ROI**: +5-10pp improvement in helpfulness ratings, +15% user engagement
